{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0a6d4ddde8f020d22769858c6343faf50cf54e29c922f4da84f6f66e8323f4169",
   "display_name": "Python 3.7.10 64-bit ('JODA': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Johdanto Datatieteeseen 2021 -practical work\n",
    "### *author: Ilpo Viertola*\n",
    "During this work, I've used [this Jupyter-notebook](https://www.kaggle.com/lunamcbride24/covid19-tweet-truth-analysis) as reference."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/ilpoviertola/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/ilpoviertola/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Normal stuff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For Twitter API\n",
    "import tweepy\n",
    "import ast\n",
    "\n",
    "# Tweet preprocessing\n",
    "import nltk\n",
    "nltk.download('stopwords')  # Download stopwords (not downloaded if up to date)\n",
    "nltk.download('wordnet')    # Download wordnet for lemmatizer (not downloaded if up to date)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import html\n",
    "import string\n",
    "\n",
    "# Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "source": [
    "## Reading the data\n",
    "**Read the data in from [Kaggle csv-files](https://www.kaggle.com/elvinagammed/covid19-fake-news-dataset-nlp) and with Twitter API.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                              tweet label\n",
       "0   1  Chinese converting to Islam after realising th...  fake\n",
       "1   2  11 out of 13 people (from the Diamond Princess...  fake\n",
       "2   3  COVID-19 Is Caused By A Bacterium, Not Virus A...  fake\n",
       "3   4  Mike Pence in RNC speech praises Donald Trumpâ€™...  fake\n",
       "4   5  6/10 Sky's @EdConwaySky explains the latest #C...  real"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Chinese converting to Islam after realising th...</td>\n      <td>fake</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>11 out of 13 people (from the Diamond Princess...</td>\n      <td>fake</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>COVID-19 Is Caused By A Bacterium, Not Virus A...</td>\n      <td>fake</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Mike Pence in RNC speech praises Donald Trumpâ€™...</td>\n      <td>fake</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>6/10 Sky's @EdConwaySky explains the latest #C...</td>\n      <td>real</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "csv_path = '/Users/ilpoviertola/OneDrive - TUNI.fi/Kurssimateriaaleja/JODA/datasets/covid19_fake_news'\n",
    "# Data to train the model with\n",
    "train_df = pd.read_csv(csv_path + '/Constraint_Train.csv')\n",
    "train_df.head()\n",
    "# Data to test the model with\n",
    "test_df = pd.read_csv(csv_path + '/Constraint_Test.csv')\n",
    "test_df.head()\n",
    "# Data to validate the model with\n",
    "val_df = pd.read_csv(csv_path + '/Constraint_Val.csv')\n",
    "val_df.head()"
   ]
  },
  {
   "source": [
    "**Next Twitter data. First lets authenticate ourselves so we can use Twitter API.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Twitter API-keys from a local file.\n",
    "key_file = open('twitter.key', 'r')\n",
    "keys = ast.literal_eval(key_file.read())\n",
    "key_file.close()\n",
    "\n",
    "auth = tweepy.OAuthHandler(keys['API'], keys['API_secret'])\n",
    "auth.set_access_token(keys['Access_token'], keys['Access_token_secret'])\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "source": [
    "**Create DataFrame where the tweets are stored and fetch tweets.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for tweets\n",
    "tweet_df = pd.DataFrame(columns=['username', 'description', 'location', 'following', 'followers', 'totaltweets', 'retweetcount', 'text', 'hashtags'])\n",
    "\n",
    "# Get tweets\n",
    "hashtag = '#covid19'\n",
    "d_since = '2021-04-07'\n",
    "limit = 250\n",
    "tweets = tweepy.Cursor(api.search, q=hashtag, lang='en', since=d_since, tweet_mode='extended').items(limit)\n",
    "tweets_list = [tweet for tweet in tweets]"
   ]
  },
  {
   "source": [
    "**Process tweets and add them to DataFrame. We'll exclude retweets.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          username                                        description  \\\n",
       "0       johnlittle  Woke Leftie with long business career. Proud t...   \n",
       "1    ziya_al_ansar  Law Student, Faculty of Law,\\nAligarh Muslim U...   \n",
       "2    kamleshkhunti  Professor of Primary Care Diabetes & Vascular ...   \n",
       "3  threadreaderapp  I'm a ðŸ¤– to help you read threads more easily. ...   \n",
       "4   JorgeLiboreiro  Now at @Euronews Brussels bureau. Interests in...   \n",
       "\n",
       "                         location following followers totaltweets  \\\n",
       "0                         Gadigal      5509      6617       22659   \n",
       "1         Aligarh, Uttar Pradesh.      1780      1836         990   \n",
       "2     University of Leicester, UK       370     10576       15951   \n",
       "3  Wherever threads are written..      1292    460853     1793412   \n",
       "4                       Bruxelles       344       401        8987   \n",
       "\n",
       "  retweetcount                                               text   hashtags  \n",
       "0            0  UK meets its #Covid19 targets vaccinating 32mi...  [Covid19]  \n",
       "1            0  Delhi CM Urges Centre To Cancel Board Exam.\\nT...  [COVID19]  \n",
       "2            0  Great results from PRINCIPLE trial\\n\\nEarly tr...  [COVID19]  \n",
       "3            0  @shreyasbt Hi! please find the unroll here: Wh...  [COVID19]  \n",
       "4            0  How the recent landmark ruling by the ECHR lay...  [COVID19]  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>username</th>\n      <th>description</th>\n      <th>location</th>\n      <th>following</th>\n      <th>followers</th>\n      <th>totaltweets</th>\n      <th>retweetcount</th>\n      <th>text</th>\n      <th>hashtags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>johnlittle</td>\n      <td>Woke Leftie with long business career. Proud t...</td>\n      <td>Gadigal</td>\n      <td>5509</td>\n      <td>6617</td>\n      <td>22659</td>\n      <td>0</td>\n      <td>UK meets its #Covid19 targets vaccinating 32mi...</td>\n      <td>[Covid19]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ziya_al_ansar</td>\n      <td>Law Student, Faculty of Law,\\nAligarh Muslim U...</td>\n      <td>Aligarh, Uttar Pradesh.</td>\n      <td>1780</td>\n      <td>1836</td>\n      <td>990</td>\n      <td>0</td>\n      <td>Delhi CM Urges Centre To Cancel Board Exam.\\nT...</td>\n      <td>[COVID19]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>kamleshkhunti</td>\n      <td>Professor of Primary Care Diabetes &amp; Vascular ...</td>\n      <td>University of Leicester, UK</td>\n      <td>370</td>\n      <td>10576</td>\n      <td>15951</td>\n      <td>0</td>\n      <td>Great results from PRINCIPLE trial\\n\\nEarly tr...</td>\n      <td>[COVID19]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>threadreaderapp</td>\n      <td>I'm a ðŸ¤– to help you read threads more easily. ...</td>\n      <td>Wherever threads are written..</td>\n      <td>1292</td>\n      <td>460853</td>\n      <td>1793412</td>\n      <td>0</td>\n      <td>@shreyasbt Hi! please find the unroll here: Wh...</td>\n      <td>[COVID19]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>JorgeLiboreiro</td>\n      <td>Now at @Euronews Brussels bureau. Interests in...</td>\n      <td>Bruxelles</td>\n      <td>344</td>\n      <td>401</td>\n      <td>8987</td>\n      <td>0</td>\n      <td>How the recent landmark ruling by the ECHR lay...</td>\n      <td>[COVID19]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "for tweet in tweets_list:\n",
    "    # Data about tweets\n",
    "    username = tweet.user.screen_name\n",
    "    description = tweet.user.description\n",
    "    location = tweet.user.location\n",
    "    following = tweet.user.friends_count\n",
    "    followers = tweet.user.followers_count\n",
    "    totaltweets = tweet.user.statuses_count\n",
    "    retweetcount = tweet.retweet_count\n",
    "    hashtags = tweet.entities['hashtags']\n",
    "    \n",
    "    # Let's ignore all retweets\n",
    "    if not tweet.retweeted and ('RT @' not in tweet.full_text):\n",
    "\n",
    "        text = tweet.full_text\n",
    "        hashtext = list()\n",
    "        for j in range(0, len(hashtags)):\n",
    "            hashtext.append(hashtags[j]['text'])\n",
    "            \n",
    "        # LisÃ¤tÃ¤Ã¤n data DataFrameen.\n",
    "        ith_tweet = [username, description, location, following, followers, totaltweets, \n",
    "                    retweetcount, text, hashtext]\n",
    "        tweet_df.loc[len(tweet_df)] = ith_tweet\n",
    "\n",
    "tweet_df.head()"
   ]
  },
  {
   "source": [
    "## Data exploration\n",
    "**Check column names**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training & validation data's columns:\n['id' 'tweet' 'label']\n['id' 'tweet' 'label']\nTest data's columns:\n['id' 'tweet']\nTwitter data's columns:\n['username' 'description' 'location' 'following' 'followers' 'totaltweets'\n 'retweetcount' 'text' 'hashtags']\n"
     ]
    }
   ],
   "source": [
    "print('Training & validation data\\'s columns:')\n",
    "print(train_df.columns.values)\n",
    "print(val_df.columns.values)\n",
    "print('Test data\\'s columns:')\n",
    "print(test_df.columns.values)\n",
    "print('Twitter data\\'s columns:')\n",
    "print(tweet_df.columns.values)"
   ]
  },
  {
   "source": [
    "**Columns are ok. Next check null-values and dataypes**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Null values in training data? False\nNull values in testing data? False\nNull values in validation data? False\nNull values in Twitter data? False\n"
     ]
    }
   ],
   "source": [
    "print('Null values in training data? ' + str(train_df.isnull().values.any()))\n",
    "print('Null values in testing data? ' + str(test_df.isnull().values.any()))\n",
    "print('Null values in validation data? ' + str(val_df.isnull().values.any()))\n",
    "print('Null values in Twitter data? ' + str(tweet_df.isnull().values.any()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Datatypes for training data: \nid        int64\ntweet    object\nlabel    object\ndtype: object\n\nDatatypes for validation data: \nid        int64\ntweet    object\nlabel    object\ndtype: object\n\nDatatypes for testing data: \nid        int64\ntweet    object\ndtype: object\n\nDatatypes for Twitter data: \nusername        object\ndescription     object\nlocation        object\nfollowing       object\nfollowers       object\ntotaltweets     object\nretweetcount    object\ntext            object\nhashtags        object\ndtype: object\n\n"
     ]
    }
   ],
   "source": [
    "print('Datatypes for training data: \\n' + str(train_df.dtypes) + '\\n')\n",
    "print('Datatypes for validation data: \\n' + str(val_df.dtypes) + '\\n')\n",
    "print('Datatypes for testing data: \\n' + str(test_df.dtypes) + '\\n')\n",
    "print('Datatypes for Twitter data: \\n' + str(tweet_df.dtypes) + '\\n')"
   ]
  },
  {
   "source": [
    "**Twitter dataset needs some datatype modifications.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "New datatypes for Twitter data: \nusername        object\ndescription     object\nlocation        object\nfollowing        int32\nfollowers        int32\ntotaltweets      int32\nretweetcount     int32\ntext            object\nhashtags        object\ndtype: object\n\n"
     ]
    }
   ],
   "source": [
    "tweet_df = tweet_df.astype({'following': 'int32', 'followers': 'int32', \n",
    "                            'totaltweets': 'int32', 'retweetcount': 'int32'})\n",
    "print('New datatypes for Twitter data: \\n' + str(tweet_df.dtypes) + '\\n')"
   ]
  },
  {
   "source": [
    "print('\\nExample tweet from training data: ')\n",
    "print(train_df['tweet'][5])\n",
    "print('\\nExample tweet from Twitter data: ')\n",
    "print(tweet_df['text'][5])"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 82,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nExample tweet from training data: \nCovid Act Now found \"on average each person in Illinois with COVID-19 is infecting 1.11 other people. Data shows that the infection growth rate has declined over time this factors in the stay-at-home order and other restrictions put in place.\" https://t.co/hhigDd24fE\n\nExample tweet from Twitter data: \nNow that the 5KM has lifted. Here are the options Corkonians #Cork #notcork #5km #COVID19 #lockdown #lockdownlifting https://t.co/mFeRbaOxJ4\n"
     ]
    }
   ]
  },
  {
   "source": [
    "**Tweets typically contain links, other people's usernames, hashtags and emojis. These must be cleaned before training the model...**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data labels: \n real    3360\nfake    3060\nName: label, dtype: int64\n\nValidation data labels: \n real    1120\nfake    1020\nName: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Training data labels: \\n', train_df['label'].value_counts())\n",
    "print('\\nValidation data labels: \\n', val_df['label'].value_counts())"
   ]
  },
  {
   "source": [
    "**Datasets are balanced, meaning they contain approximately as much fake and real news. These values must be binarycoded in the future.**  \n",
    "  \n",
    "## Tweet preprocessing aka. feature extraction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "puncs = string.punctuation\n",
    "stopws = stopwords.words('english')\n",
    "print(puncs)\n",
    "print(stopws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_cleaner(tweets):\n",
    "    for i in range(0, len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "\n",
    "        emoji_pattern = re.compile(pattern = '['\n",
    "            u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "            u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "            u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "            u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "            u'\\U00002702-\\U000027B0'\n",
    "            u'\\U000024C2-\\U0001F251'\n",
    "            u'\\U0001f926-\\U0001f937'\n",
    "            u'\\U00010000-\\U0010ffff'\n",
    "            u'\\u2640-\\u2642'\n",
    "            u'\\u2600-\\u2B55'\n",
    "            u'\\u200d'\n",
    "            u'\\u23cf'\n",
    "            u'\\u23e9'\n",
    "            u'\\u231a'\n",
    "            u'\\ufe0f'  # dingbats\n",
    "            u'\\u3030'\n",
    "            ']+', flags = re.UNICODE)\n",
    "\n",
    "        tweet = html.unescape(tweet)    # Remove leftover HTML elements\n",
    "        tweet = re.sub(r'@\\w+', ' ', tweet) # Remove mentions to other people\n",
    "        tweet = re.sub(r'http\\S+', ' ', tweet)  # Remove links\n",
    "        tweet = emoji_pattern.sub(r'', tweet)   # Remove emojis\n",
    "        \n",
    "        tweet = ''.join([punc for punc in tweet if not punc in puncs])   # Remove punctuation\n",
    "        tweet = tweet.lower()   # Lowercase text\n",
    "    \n",
    "        tweetWord = tweet.split()   # Split to words\n",
    "\n",
    "        lemmatiser = WordNetLemmatizer()\n",
    "        tweetWord = [lemmatiser.lemmatize(word, pos='v') for word in tweetWord] # Lemmatize words\n",
    "\n",
    "        tweets[i] = ''.join([word + ' ' for word in tweetWord if not word in stopws]) # Exclude stopwords\n",
    "        \n",
    "    return tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                              tweet label  \\\n",
       "0   1  The CDC currently reports 99031 deaths. In gen...  real   \n",
       "1   2  States reported 1121 deaths a small rise from ...  real   \n",
       "2   3  Politically Correct Woman (Almost) Uses Pandem...  fake   \n",
       "3   4  #IndiaFightsCorona: We have 1524 #COVID testin...  real   \n",
       "4   5  Populous states can generate large case counts...  real   \n",
       "\n",
       "                                         clean_tweet  is_real  \\\n",
       "0  cdc currently report 99031 deaths general disc...        1   \n",
       "1  state report 1121 deaths small rise last tuesd...        1   \n",
       "2  politically correct woman almost use pandemic ...        0   \n",
       "3  indiafightscorona 1524 covid test laboratories...        1   \n",
       "4  populous state generate large case count look ...        1   \n",
       "\n",
       "                                      tweet_sequence  \n",
       "0  [93, 199, 6, 8639, 10, 659, 4638, 90, 403, 386...  \n",
       "1  [7, 6, 8641, 10, 645, 132, 47, 1038, 2483, 7, ...  \n",
       "2  [5933, 1150, 387, 471, 37, 23, 2484, 3889, 197...  \n",
       "3  [19, 8642, 14, 3, 210, 16, 3009, 213, 42, 5934...  \n",
       "4  [5935, 7, 2110, 440, 2, 403, 196, 5, 2, 100, 5...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n      <th>clean_tweet</th>\n      <th>is_real</th>\n      <th>tweet_sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>The CDC currently reports 99031 deaths. In gen...</td>\n      <td>real</td>\n      <td>cdc currently report 99031 deaths general disc...</td>\n      <td>1</td>\n      <td>[93, 199, 6, 8639, 10, 659, 4638, 90, 403, 386...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>States reported 1121 deaths a small rise from ...</td>\n      <td>real</td>\n      <td>state report 1121 deaths small rise last tuesd...</td>\n      <td>1</td>\n      <td>[7, 6, 8641, 10, 645, 132, 47, 1038, 2483, 7, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n      <td>fake</td>\n      <td>politically correct woman almost use pandemic ...</td>\n      <td>0</td>\n      <td>[5933, 1150, 387, 471, 37, 23, 2484, 3889, 197...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n      <td>real</td>\n      <td>indiafightscorona 1524 covid test laboratories...</td>\n      <td>1</td>\n      <td>[19, 8642, 14, 3, 210, 16, 3009, 213, 42, 5934...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Populous states can generate large case counts...</td>\n      <td>real</td>\n      <td>populous state generate large case count look ...</td>\n      <td>1</td>\n      <td>[5935, 7, 2110, 440, 2, 403, 196, 5, 2, 100, 5...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "train_df['clean_tweet'] = tweet_cleaner(train_df['tweet'].copy())\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                              tweet  \\\n",
       "0   1  Our daily update is published. States reported...   \n",
       "1   2             Alfalfa is the only cure for COVID-19.   \n",
       "2   3  President Trump Asked What He Would Do If He W...   \n",
       "3   4  States reported 630 deaths. We are still seein...   \n",
       "4   5  This is the sixth time a global health emergen...   \n",
       "\n",
       "                                         clean_tweet  \\\n",
       "0  daily update publish state report 734k test 39...   \n",
       "1                              alfalfa cure covid19    \n",
       "2  president trump ask would catch coronavirus do...   \n",
       "3  state report 630 deaths still see solid nation...   \n",
       "4  sixth time global health emergency declare int...   \n",
       "\n",
       "                                      tweet_sequence  \n",
       "0  [44, 18, 94, 7, 6, 16494, 3, 3658, 5, 2, 5349,...  \n",
       "1  [16495, 79, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "2  [84, 45, 322, 159, 623, 4, 304, 4, 0, 0, 0, 0,...  \n",
       "3  [7, 6, 16496, 10, 124, 27, 2127, 145, 426, 90,...  \n",
       "4  [2948, 33, 334, 15, 377, 913, 542, 15, 2387, 1...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>clean_tweet</th>\n      <th>tweet_sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Our daily update is published. States reported...</td>\n      <td>daily update publish state report 734k test 39...</td>\n      <td>[44, 18, 94, 7, 6, 16494, 3, 3658, 5, 2, 5349,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Alfalfa is the only cure for COVID-19.</td>\n      <td>alfalfa cure covid19</td>\n      <td>[16495, 79, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>President Trump Asked What He Would Do If He W...</td>\n      <td>president trump ask would catch coronavirus do...</td>\n      <td>[84, 45, 322, 159, 623, 4, 304, 4, 0, 0, 0, 0,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>States reported 630 deaths. We are still seein...</td>\n      <td>state report 630 deaths still see solid nation...</td>\n      <td>[7, 6, 16496, 10, 124, 27, 2127, 145, 426, 90,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>This is the sixth time a global health emergen...</td>\n      <td>sixth time global health emergency declare int...</td>\n      <td>[2948, 33, 334, 15, 377, 913, 542, 15, 2387, 1...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "test_df['clean_tweet'] = tweet_cleaner(test_df['tweet'].copy())\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                              tweet label  \\\n",
       "0   1  Chinese converting to Islam after realising th...  fake   \n",
       "1   2  11 out of 13 people (from the Diamond Princess...  fake   \n",
       "2   3  COVID-19 Is Caused By A Bacterium, Not Virus A...  fake   \n",
       "3   4  Mike Pence in RNC speech praises Donald Trumpâ€™...  fake   \n",
       "4   5  6/10 Sky's @EdConwaySky explains the latest #C...  real   \n",
       "\n",
       "                                         clean_tweet  is_real  \\\n",
       "0  chinese convert islam realise muslim affect co...        0   \n",
       "1  11 13 people diamond princess cruise ship inti...        0   \n",
       "2       covid19 cause bacterium virus treat aspirin         0   \n",
       "3  mike pence rnc speech praise donald trumpâ€™s co...        0   \n",
       "4  610 sky explain latest covid19 data government...        1   \n",
       "\n",
       "                                      tweet_sequence  \n",
       "0  [187, 1818, 2657, 3127, 701, 284, 4, 6875, 92,...  \n",
       "1  [449, 504, 8, 4023, 3508, 1323, 817, 14630, 3,...  \n",
       "2  [1, 117, 2372, 24, 180, 2350, 0, 0, 0, 0, 0, 0...  \n",
       "3  [1892, 1722, 6305, 2840, 1907, 197, 1352, 1, 5...  \n",
       "4  [14632, 754, 492, 105, 1, 32, 78, 2246, 29, 4,...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n      <th>clean_tweet</th>\n      <th>is_real</th>\n      <th>tweet_sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Chinese converting to Islam after realising th...</td>\n      <td>fake</td>\n      <td>chinese convert islam realise muslim affect co...</td>\n      <td>0</td>\n      <td>[187, 1818, 2657, 3127, 701, 284, 4, 6875, 92,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>11 out of 13 people (from the Diamond Princess...</td>\n      <td>fake</td>\n      <td>11 13 people diamond princess cruise ship inti...</td>\n      <td>0</td>\n      <td>[449, 504, 8, 4023, 3508, 1323, 817, 14630, 3,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>COVID-19 Is Caused By A Bacterium, Not Virus A...</td>\n      <td>fake</td>\n      <td>covid19 cause bacterium virus treat aspirin</td>\n      <td>0</td>\n      <td>[1, 117, 2372, 24, 180, 2350, 0, 0, 0, 0, 0, 0...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Mike Pence in RNC speech praises Donald Trumpâ€™...</td>\n      <td>fake</td>\n      <td>mike pence rnc speech praise donald trumpâ€™s co...</td>\n      <td>0</td>\n      <td>[1892, 1722, 6305, 2840, 1907, 197, 1352, 1, 5...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>6/10 Sky's @EdConwaySky explains the latest #C...</td>\n      <td>real</td>\n      <td>610 sky explain latest covid19 data government...</td>\n      <td>1</td>\n      <td>[14632, 754, 492, 105, 1, 32, 78, 2246, 29, 4,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "val_df['clean_tweet'] = tweet_cleaner(val_df['tweet'].copy())\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          username                                        description  \\\n",
       "0       johnlittle  Woke Leftie with long business career. Proud t...   \n",
       "1    ziya_al_ansar  Law Student, Faculty of Law,\\nAligarh Muslim U...   \n",
       "2    kamleshkhunti  Professor of Primary Care Diabetes & Vascular ...   \n",
       "3  threadreaderapp  I'm a ðŸ¤– to help you read threads more easily. ...   \n",
       "4   JorgeLiboreiro  Now at @Euronews Brussels bureau. Interests in...   \n",
       "\n",
       "                         location  following  followers  totaltweets  \\\n",
       "0                         Gadigal       5509       6617        22659   \n",
       "1         Aligarh, Uttar Pradesh.       1780       1836          990   \n",
       "2     University of Leicester, UK        370      10576        15951   \n",
       "3  Wherever threads are written..       1292     460853      1793412   \n",
       "4                       Bruxelles        344        401         8987   \n",
       "\n",
       "   retweetcount                                               text   hashtags  \\\n",
       "0             0  UK meets its #Covid19 targets vaccinating 32mi...  [Covid19]   \n",
       "1             0  Delhi CM Urges Centre To Cancel Board Exam.\\nT...  [COVID19]   \n",
       "2             0  Great results from PRINCIPLE trial\\n\\nEarly tr...  [COVID19]   \n",
       "3             0  @shreyasbt Hi! please find the unroll here: Wh...  [COVID19]   \n",
       "4             0  How the recent landmark ruling by the ECHR lay...  [COVID19]   \n",
       "\n",
       "                                         clean_tweet  \\\n",
       "0  uk meet covid19 target vaccinate 32million top...   \n",
       "1  delhi cm urge centre cancel board exam today u...   \n",
       "2  great result principle trial early treatment r...   \n",
       "3  hi please find unroll know far covid19 1 fresh...   \n",
       "4  recent landmark rule echr lay grind mandatory ...   \n",
       "\n",
       "                                      tweet_sequence  \n",
       "0  [173, 474, 1, 1278, 1129, 18455, 647, 1798, 20...  \n",
       "1  [470, 1509, 609, 369, 1103, 1675, 3170, 17, 60...  \n",
       "2  [808, 135, 8628, 545, 349, 205, 4775, 37, 1403...  \n",
       "3  [1730, 191, 64, 18459, 108, 275, 1, 38, 1844, ...  \n",
       "4  [367, 1923, 325, 18462, 1847, 1349, 1081, 1, 8...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>username</th>\n      <th>description</th>\n      <th>location</th>\n      <th>following</th>\n      <th>followers</th>\n      <th>totaltweets</th>\n      <th>retweetcount</th>\n      <th>text</th>\n      <th>hashtags</th>\n      <th>clean_tweet</th>\n      <th>tweet_sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>johnlittle</td>\n      <td>Woke Leftie with long business career. Proud t...</td>\n      <td>Gadigal</td>\n      <td>5509</td>\n      <td>6617</td>\n      <td>22659</td>\n      <td>0</td>\n      <td>UK meets its #Covid19 targets vaccinating 32mi...</td>\n      <td>[Covid19]</td>\n      <td>uk meet covid19 target vaccinate 32million top...</td>\n      <td>[173, 474, 1, 1278, 1129, 18455, 647, 1798, 20...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ziya_al_ansar</td>\n      <td>Law Student, Faculty of Law,\\nAligarh Muslim U...</td>\n      <td>Aligarh, Uttar Pradesh.</td>\n      <td>1780</td>\n      <td>1836</td>\n      <td>990</td>\n      <td>0</td>\n      <td>Delhi CM Urges Centre To Cancel Board Exam.\\nT...</td>\n      <td>[COVID19]</td>\n      <td>delhi cm urge centre cancel board exam today u...</td>\n      <td>[470, 1509, 609, 369, 1103, 1675, 3170, 17, 60...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>kamleshkhunti</td>\n      <td>Professor of Primary Care Diabetes &amp; Vascular ...</td>\n      <td>University of Leicester, UK</td>\n      <td>370</td>\n      <td>10576</td>\n      <td>15951</td>\n      <td>0</td>\n      <td>Great results from PRINCIPLE trial\\n\\nEarly tr...</td>\n      <td>[COVID19]</td>\n      <td>great result principle trial early treatment r...</td>\n      <td>[808, 135, 8628, 545, 349, 205, 4775, 37, 1403...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>threadreaderapp</td>\n      <td>I'm a ðŸ¤– to help you read threads more easily. ...</td>\n      <td>Wherever threads are written..</td>\n      <td>1292</td>\n      <td>460853</td>\n      <td>1793412</td>\n      <td>0</td>\n      <td>@shreyasbt Hi! please find the unroll here: Wh...</td>\n      <td>[COVID19]</td>\n      <td>hi please find unroll know far covid19 1 fresh...</td>\n      <td>[1730, 191, 64, 18459, 108, 275, 1, 38, 1844, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>JorgeLiboreiro</td>\n      <td>Now at @Euronews Brussels bureau. Interests in...</td>\n      <td>Bruxelles</td>\n      <td>344</td>\n      <td>401</td>\n      <td>8987</td>\n      <td>0</td>\n      <td>How the recent landmark ruling by the ECHR lay...</td>\n      <td>[COVID19]</td>\n      <td>recent landmark rule echr lay grind mandatory ...</td>\n      <td>[367, 1923, 325, 18462, 1847, 1349, 1081, 1, 8...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "tweet_df['clean_tweet'] = tweet_cleaner(tweet_df['text'].copy())\n",
    "tweet_df.head()"
   ]
  },
  {
   "source": [
    "**Remove rows that have blank clean_tweets. (This could be the case if the tweet only contained e.g. a link)**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['clean_tweet'].replace('', np.nan, inplace=True)\n",
    "test_df['clean_tweet'].replace('', np.nan, inplace=True)\n",
    "val_df['clean_tweet'].replace('', np.nan, inplace=True)\n",
    "tweet_df['clean_tweet'].replace('', np.nan, inplace=True)\n",
    "\n",
    "train_df.dropna(subset=['clean_tweet'], inplace=True)\n",
    "test_df.dropna(subset=['clean_tweet'], inplace=True)\n",
    "val_df.dropna(subset=['clean_tweet'], inplace=True)\n",
    "tweet_df.dropna(subset=['clean_tweet'], inplace=True)"
   ]
  },
  {
   "source": [
    "**Compare \"dirty\" and \"clean\" tweet**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Some dirty tweet:\n Thirty-nine GPs and specialists have written to the BMJ calling for action on long COVID. https://t.co/4Y5kGv3pF3 https://t.co/jTc1OucOmw\n\nClean version:\n thirtynine gps specialists write bmj call action long covid \n"
     ]
    }
   ],
   "source": [
    "print('Some dirty tweet:\\n', train_df['tweet'][150])\n",
    "print('\\nClean version:\\n', train_df['clean_tweet'][150])"
   ]
  },
  {
   "source": [
    "**Binarycode label-colum values to is_real-column in train_df and val_df. 0 = fake, 1 = real**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                              tweet label  \\\n",
       "0   1  Chinese converting to Islam after realising th...  fake   \n",
       "1   2  11 out of 13 people (from the Diamond Princess...  fake   \n",
       "2   3  COVID-19 Is Caused By A Bacterium, Not Virus A...  fake   \n",
       "3   4  Mike Pence in RNC speech praises Donald Trumpâ€™...  fake   \n",
       "4   5  6/10 Sky's @EdConwaySky explains the latest #C...  real   \n",
       "\n",
       "                                         clean_tweet  is_real  \\\n",
       "0  chinese convert islam realise muslim affect co...        0   \n",
       "1  11 13 people diamond princess cruise ship inti...        0   \n",
       "2       covid19 cause bacterium virus treat aspirin         0   \n",
       "3  mike pence rnc speech praise donald trumpâ€™s co...        0   \n",
       "4  610 sky explain latest covid19 data government...        1   \n",
       "\n",
       "                                      tweet_sequence  \n",
       "0  [187, 1818, 2657, 3127, 701, 284, 4, 6875, 92,...  \n",
       "1  [449, 504, 8, 4023, 3508, 1323, 817, 14630, 3,...  \n",
       "2  [1, 117, 2372, 24, 180, 2350, 0, 0, 0, 0, 0, 0...  \n",
       "3  [1892, 1722, 6305, 2840, 1907, 197, 1352, 1, 5...  \n",
       "4  [14632, 754, 492, 105, 1, 32, 78, 2246, 29, 4,...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n      <th>clean_tweet</th>\n      <th>is_real</th>\n      <th>tweet_sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Chinese converting to Islam after realising th...</td>\n      <td>fake</td>\n      <td>chinese convert islam realise muslim affect co...</td>\n      <td>0</td>\n      <td>[187, 1818, 2657, 3127, 701, 284, 4, 6875, 92,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>11 out of 13 people (from the Diamond Princess...</td>\n      <td>fake</td>\n      <td>11 13 people diamond princess cruise ship inti...</td>\n      <td>0</td>\n      <td>[449, 504, 8, 4023, 3508, 1323, 817, 14630, 3,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>COVID-19 Is Caused By A Bacterium, Not Virus A...</td>\n      <td>fake</td>\n      <td>covid19 cause bacterium virus treat aspirin</td>\n      <td>0</td>\n      <td>[1, 117, 2372, 24, 180, 2350, 0, 0, 0, 0, 0, 0...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Mike Pence in RNC speech praises Donald Trumpâ€™...</td>\n      <td>fake</td>\n      <td>mike pence rnc speech praise donald trumpâ€™s co...</td>\n      <td>0</td>\n      <td>[1892, 1722, 6305, 2840, 1907, 197, 1352, 1, 5...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>6/10 Sky's @EdConwaySky explains the latest #C...</td>\n      <td>real</td>\n      <td>610 sky explain latest covid19 data government...</td>\n      <td>1</td>\n      <td>[14632, 754, 492, 105, 1, 32, 78, 2246, 29, 4,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "source": [
    "train_df['is_real'] = pd.get_dummies(train_df['label'])['real']\n",
    "val_df['is_real'] = pd.get_dummies(val_df['label'])['real']\n",
    "val_df.head()"
   ]
  },
  {
   "source": [
    "**Export dataframes to csv-files**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(csv_path + '/modified_datasets'):\n",
    "    os.makedirs(csv_path + '/modified_datasets')\n",
    "\n",
    "train_df.to_csv(csv_path + '/modified_datasets/train.csv')\n",
    "test_df.to_csv(csv_path + '/modified_datasets/test.csv')\n",
    "val_df.to_csv(csv_path + '/modified_datasets/val.csv')\n",
    "tweet_df.to_csv(csv_path + '/modified_datasets/tweets.csv')"
   ]
  },
  {
   "source": [
    "### Tokenization and padding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Amount of tweets: 10754\n"
     ]
    }
   ],
   "source": [
    "train_clean = train_df['clean_tweet'].copy()\n",
    "test_clean = test_df['clean_tweet'].copy()\n",
    "val_clean = val_df['clean_tweet'].copy()\n",
    "tweet_clean = tweet_df['clean_tweet'].copy()\n",
    "\n",
    "all_clean = train_clean.append(val_clean, ignore_index=True).append(test_clean, ignore_index=True).append(tweet_clean, ignore_index=True)\n",
    "print('Amount of tweets:', len(all_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Tokenizer contains an integer value for every individual word that is in all_clean dataset.\n",
    "# This is done because neural networks do not operate on words, so\n",
    "# we need to map a word to a integer.\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns a list of integers. Each integer is mapped to a certain word.\n",
    "# This list is the clean_tweet coded to integer values.\n",
    "# Every sequence aka. list must be same length so we need to also pad shorter sequences with 0s.\n",
    "def tokenize_tweet(tweets):\n",
    "    texts = tokenizer.texts_to_sequences(tweets) # Convert clean_tweet to sequence of integers.\n",
    "    texts = pad_sequences(texts, padding='post') # Pad the sequences with 0s so lenghts match.\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                              tweet label  \\\n",
       "0   1  The CDC currently reports 99031 deaths. In gen...  real   \n",
       "1   2  States reported 1121 deaths a small rise from ...  real   \n",
       "2   3  Politically Correct Woman (Almost) Uses Pandem...  fake   \n",
       "3   4  #IndiaFightsCorona: We have 1524 #COVID testin...  real   \n",
       "4   5  Populous states can generate large case counts...  real   \n",
       "\n",
       "                                         clean_tweet  is_real  \\\n",
       "0  cdc currently report 99031 deaths general disc...        1   \n",
       "1  state report 1121 deaths small rise last tuesd...        1   \n",
       "2  politically correct woman almost use pandemic ...        0   \n",
       "3  indiafightscorona 1524 covid test laboratories...        1   \n",
       "4  populous state generate large case count look ...        1   \n",
       "\n",
       "                                      tweet_sequence  \n",
       "0  [93, 199, 6, 8631, 10, 645, 4637, 90, 403, 386...  \n",
       "1  [7, 6, 8633, 10, 646, 132, 47, 1038, 2482, 7, ...  \n",
       "2  [5926, 1152, 387, 471, 37, 23, 2483, 3888, 197...  \n",
       "3  [19, 8634, 14, 3, 210, 16, 3008, 213, 42, 5927...  \n",
       "4  [5928, 7, 2110, 440, 2, 403, 196, 5, 2, 100, 5...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n      <th>clean_tweet</th>\n      <th>is_real</th>\n      <th>tweet_sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>The CDC currently reports 99031 deaths. In gen...</td>\n      <td>real</td>\n      <td>cdc currently report 99031 deaths general disc...</td>\n      <td>1</td>\n      <td>[93, 199, 6, 8631, 10, 645, 4637, 90, 403, 386...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>States reported 1121 deaths a small rise from ...</td>\n      <td>real</td>\n      <td>state report 1121 deaths small rise last tuesd...</td>\n      <td>1</td>\n      <td>[7, 6, 8633, 10, 646, 132, 47, 1038, 2482, 7, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n      <td>fake</td>\n      <td>politically correct woman almost use pandemic ...</td>\n      <td>0</td>\n      <td>[5926, 1152, 387, 471, 37, 23, 2483, 3888, 197...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n      <td>real</td>\n      <td>indiafightscorona 1524 covid test laboratories...</td>\n      <td>1</td>\n      <td>[19, 8634, 14, 3, 210, 16, 3008, 213, 42, 5927...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Populous states can generate large case counts...</td>\n      <td>real</td>\n      <td>populous state generate large case count look ...</td>\n      <td>1</td>\n      <td>[5928, 7, 2110, 440, 2, 403, 196, 5, 2, 100, 5...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "source": [
    "texts_train = tokenize_tweet(train_df[\"clean_tweet\"].copy()) # Collect the tweet sequences\n",
    "train_df[\"tweet_sequence\"] = list(texts_train)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                              tweet  \\\n",
       "0   1  Our daily update is published. States reported...   \n",
       "1   2             Alfalfa is the only cure for COVID-19.   \n",
       "2   3  President Trump Asked What He Would Do If He W...   \n",
       "3   4  States reported 630 deaths. We are still seein...   \n",
       "4   5  This is the sixth time a global health emergen...   \n",
       "\n",
       "                                         clean_tweet  \\\n",
       "0  daily update publish state report 734k test 39...   \n",
       "1                              alfalfa cure covid19    \n",
       "2  president trump ask would catch coronavirus do...   \n",
       "3  state report 630 deaths still see solid nation...   \n",
       "4  sixth time global health emergency declare int...   \n",
       "\n",
       "                                      tweet_sequence  \n",
       "0  [44, 18, 94, 7, 6, 16453, 3, 3657, 5, 2, 5344,...  \n",
       "1  [16454, 79, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "2  [84, 45, 322, 159, 623, 4, 304, 4, 0, 0, 0, 0,...  \n",
       "3  [7, 6, 16455, 10, 125, 27, 2127, 145, 426, 90,...  \n",
       "4  [2947, 33, 334, 15, 377, 913, 538, 15, 2386, 1...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>clean_tweet</th>\n      <th>tweet_sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Our daily update is published. States reported...</td>\n      <td>daily update publish state report 734k test 39...</td>\n      <td>[44, 18, 94, 7, 6, 16453, 3, 3657, 5, 2, 5344,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Alfalfa is the only cure for COVID-19.</td>\n      <td>alfalfa cure covid19</td>\n      <td>[16454, 79, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>President Trump Asked What He Would Do If He W...</td>\n      <td>president trump ask would catch coronavirus do...</td>\n      <td>[84, 45, 322, 159, 623, 4, 304, 4, 0, 0, 0, 0,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>States reported 630 deaths. We are still seein...</td>\n      <td>state report 630 deaths still see solid nation...</td>\n      <td>[7, 6, 16455, 10, 125, 27, 2127, 145, 426, 90,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>This is the sixth time a global health emergen...</td>\n      <td>sixth time global health emergency declare int...</td>\n      <td>[2947, 33, 334, 15, 377, 913, 538, 15, 2386, 1...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 116
    }
   ],
   "source": [
    "texts_test = tokenize_tweet(test_df[\"clean_tweet\"].copy())\n",
    "test_df[\"tweet_sequence\"] = list(texts_test)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                              tweet label  \\\n",
       "0   1  Chinese converting to Islam after realising th...  fake   \n",
       "1   2  11 out of 13 people (from the Diamond Princess...  fake   \n",
       "2   3  COVID-19 Is Caused By A Bacterium, Not Virus A...  fake   \n",
       "3   4  Mike Pence in RNC speech praises Donald Trumpâ€™...  fake   \n",
       "4   5  6/10 Sky's @EdConwaySky explains the latest #C...  real   \n",
       "\n",
       "                                         clean_tweet  is_real  \\\n",
       "0  chinese convert islam realise muslim affect co...        0   \n",
       "1  11 13 people diamond princess cruise ship inti...        0   \n",
       "2       covid19 cause bacterium virus treat aspirin         0   \n",
       "3  mike pence rnc speech praise donald trumpâ€™s co...        0   \n",
       "4  610 sky explain latest covid19 data government...        1   \n",
       "\n",
       "                                      tweet_sequence  \n",
       "0  [187, 1818, 2657, 3126, 701, 284, 4, 6868, 92,...  \n",
       "1  [449, 504, 8, 4022, 3508, 1323, 817, 14595, 3,...  \n",
       "2  [1, 117, 2371, 24, 180, 2350, 0, 0, 0, 0, 0, 0...  \n",
       "3  [1893, 1722, 6298, 2839, 1908, 197, 1352, 1, 5...  \n",
       "4  [14597, 754, 492, 105, 1, 32, 78, 2246, 29, 4,...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n      <th>clean_tweet</th>\n      <th>is_real</th>\n      <th>tweet_sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Chinese converting to Islam after realising th...</td>\n      <td>fake</td>\n      <td>chinese convert islam realise muslim affect co...</td>\n      <td>0</td>\n      <td>[187, 1818, 2657, 3126, 701, 284, 4, 6868, 92,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>11 out of 13 people (from the Diamond Princess...</td>\n      <td>fake</td>\n      <td>11 13 people diamond princess cruise ship inti...</td>\n      <td>0</td>\n      <td>[449, 504, 8, 4022, 3508, 1323, 817, 14595, 3,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>COVID-19 Is Caused By A Bacterium, Not Virus A...</td>\n      <td>fake</td>\n      <td>covid19 cause bacterium virus treat aspirin</td>\n      <td>0</td>\n      <td>[1, 117, 2371, 24, 180, 2350, 0, 0, 0, 0, 0, 0...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Mike Pence in RNC speech praises Donald Trumpâ€™...</td>\n      <td>fake</td>\n      <td>mike pence rnc speech praise donald trumpâ€™s co...</td>\n      <td>0</td>\n      <td>[1893, 1722, 6298, 2839, 1908, 197, 1352, 1, 5...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>6/10 Sky's @EdConwaySky explains the latest #C...</td>\n      <td>real</td>\n      <td>610 sky explain latest covid19 data government...</td>\n      <td>1</td>\n      <td>[14597, 754, 492, 105, 1, 32, 78, 2246, 29, 4,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "source": [
    "texts_val = tokenize_tweet(val_df[\"clean_tweet\"].copy())\n",
    "val_df[\"tweet_sequence\"] = list(texts_val)\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          username                                        description  \\\n",
       "0       johnlittle  Woke Leftie with long business career. Proud t...   \n",
       "1    ziya_al_ansar  Law Student, Faculty of Law,\\nAligarh Muslim U...   \n",
       "2    kamleshkhunti  Professor of Primary Care Diabetes & Vascular ...   \n",
       "3  threadreaderapp  I'm a ðŸ¤– to help you read threads more easily. ...   \n",
       "4   JorgeLiboreiro  Now at @Euronews Brussels bureau. Interests in...   \n",
       "\n",
       "                         location  following  followers  totaltweets  \\\n",
       "0                         Gadigal       5509       6617        22659   \n",
       "1         Aligarh, Uttar Pradesh.       1780       1836          990   \n",
       "2     University of Leicester, UK        370      10576        15951   \n",
       "3  Wherever threads are written..       1292     460853      1793412   \n",
       "4                       Bruxelles        344        401         8987   \n",
       "\n",
       "   retweetcount                                               text   hashtags  \\\n",
       "0             0  UK meets its #Covid19 targets vaccinating 32mi...  [Covid19]   \n",
       "1             0  Delhi CM Urges Centre To Cancel Board Exam.\\nT...  [COVID19]   \n",
       "2             0  Great results from PRINCIPLE trial\\n\\nEarly tr...  [COVID19]   \n",
       "3             0  @shreyasbt Hi! please find the unroll here: Wh...  [COVID19]   \n",
       "4             0  How the recent landmark ruling by the ECHR lay...  [COVID19]   \n",
       "\n",
       "                                         clean_tweet  \\\n",
       "0  uk meet covid19 target vaccinate 32million top...   \n",
       "1  delhi cm urge centre cancel board exam today u...   \n",
       "2  great result principle trial early treatment r...   \n",
       "3  hi please find unroll know far covid19 1 fresh...   \n",
       "4  recent landmark rule echr lay grind mandatory ...   \n",
       "\n",
       "                                      tweet_sequence  \n",
       "0  [173, 474, 1, 1278, 1129, 18405, 648, 1798, 20...  \n",
       "1  [470, 1509, 609, 369, 1103, 1675, 3170, 17, 60...  \n",
       "2  [808, 135, 8620, 545, 349, 205, 4774, 37, 1403...  \n",
       "3  [1730, 191, 64, 18409, 108, 275, 1, 38, 1844, ...  \n",
       "4  [367, 1924, 327, 18411, 1847, 1349, 1081, 1, 8...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>username</th>\n      <th>description</th>\n      <th>location</th>\n      <th>following</th>\n      <th>followers</th>\n      <th>totaltweets</th>\n      <th>retweetcount</th>\n      <th>text</th>\n      <th>hashtags</th>\n      <th>clean_tweet</th>\n      <th>tweet_sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>johnlittle</td>\n      <td>Woke Leftie with long business career. Proud t...</td>\n      <td>Gadigal</td>\n      <td>5509</td>\n      <td>6617</td>\n      <td>22659</td>\n      <td>0</td>\n      <td>UK meets its #Covid19 targets vaccinating 32mi...</td>\n      <td>[Covid19]</td>\n      <td>uk meet covid19 target vaccinate 32million top...</td>\n      <td>[173, 474, 1, 1278, 1129, 18405, 648, 1798, 20...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ziya_al_ansar</td>\n      <td>Law Student, Faculty of Law,\\nAligarh Muslim U...</td>\n      <td>Aligarh, Uttar Pradesh.</td>\n      <td>1780</td>\n      <td>1836</td>\n      <td>990</td>\n      <td>0</td>\n      <td>Delhi CM Urges Centre To Cancel Board Exam.\\nT...</td>\n      <td>[COVID19]</td>\n      <td>delhi cm urge centre cancel board exam today u...</td>\n      <td>[470, 1509, 609, 369, 1103, 1675, 3170, 17, 60...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>kamleshkhunti</td>\n      <td>Professor of Primary Care Diabetes &amp; Vascular ...</td>\n      <td>University of Leicester, UK</td>\n      <td>370</td>\n      <td>10576</td>\n      <td>15951</td>\n      <td>0</td>\n      <td>Great results from PRINCIPLE trial\\n\\nEarly tr...</td>\n      <td>[COVID19]</td>\n      <td>great result principle trial early treatment r...</td>\n      <td>[808, 135, 8620, 545, 349, 205, 4774, 37, 1403...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>threadreaderapp</td>\n      <td>I'm a ðŸ¤– to help you read threads more easily. ...</td>\n      <td>Wherever threads are written..</td>\n      <td>1292</td>\n      <td>460853</td>\n      <td>1793412</td>\n      <td>0</td>\n      <td>@shreyasbt Hi! please find the unroll here: Wh...</td>\n      <td>[COVID19]</td>\n      <td>hi please find unroll know far covid19 1 fresh...</td>\n      <td>[1730, 191, 64, 18409, 108, 275, 1, 38, 1844, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>JorgeLiboreiro</td>\n      <td>Now at @Euronews Brussels bureau. Interests in...</td>\n      <td>Bruxelles</td>\n      <td>344</td>\n      <td>401</td>\n      <td>8987</td>\n      <td>0</td>\n      <td>How the recent landmark ruling by the ECHR lay...</td>\n      <td>[COVID19]</td>\n      <td>recent landmark rule echr lay grind mandatory ...</td>\n      <td>[367, 1924, 327, 18411, 1847, 1349, 1081, 1, 8...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "source": [
    "texts_tweets = tokenize_tweet(tweet_df[\"clean_tweet\"].copy())\n",
    "tweet_df[\"tweet_sequence\"] = list(texts_tweets)\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Some clean tweet: new numerous covid19 outbreaks recent cruise ship voyage extend previous sail order prevent spread covid19 among crew onboard \n\nSame tweet's encoded sequence: [   5 6082    1  642  367 1323  817 8920  894  748 4770  273  140   31\n    1  365 2537 6083    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0]\n\nSequence transformed back to normal text: ['new numerous covid19 outbreaks recent cruise ship voyage extend previous sail order prevent spread covid19 among crew onboard']\n"
     ]
    }
   ],
   "source": [
    "print('Some clean tweet:', train_df['clean_tweet'][300])\n",
    "print('\\nSame tweet\\'s encoded sequence:', train_df['tweet_sequence'][300])\n",
    "print('\\nSequence transformed back to normal text:', tokenizer.sequences_to_texts([train_df['tweet_sequence'][300]]))"
   ]
  },
  {
   "source": [
    "## Describing the data\n",
    "In this section, we will take a look inside the datasets we're going to use in this work."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "17459, 'arctkskynhri': 17460, '425k': 17461, '654k': 17462, 'utterly': 17463, 'nearworstcaseeconomic': 17464, 'lotteries': 17465, '1228': 17466, 'operationalised': 17467, 'commend': 17468, 'balram': 17469, 'bhargava': 17470, 'coronaviâ€¦': 17471, 'vacination': 17472, 'bioethics': 17473, 'yatris': 17474, 'vaishno': 17475, 'reset': 17476, 'elderlyill': 17477, 'â€œuseless': 17478, 'feedersâ€': 17479, 'nwo': 17480, 'zonein': 17481, 'homecarry': 17482, 'bystander': 17483, 'cpr': 17484, 'â€˜amavasyaâ€™': 17485, 'darkest': 17486, 'clap': 17487, 'shankh': 17488, 'vibrations': 17489, 'moon': 17490, 'â€˜nakshatraâ€™': 17491, 'revati': 17492, 'vibration': 17493, 'circulationâ€': 17494, 'infectiousdisease': 17495, 'asshole': 17496, 'earlypandemic': 17497, 'outnumber': 17498, 'sajtha': 17499, 'masha': 17500, 'llah': 17501, '49769': 17502, 'oxygensaturation': 17503, '950680': 17504, 'distanceâ€¦': 17505, '1280': 17506, 'abusedeterrent': 17507, '8772': 17508, '7238': 17509, '6008': 17510, '4597': 17511, 'band': 17512, 'hospitals\\u200b': 17513, 'rideshares': 17514, 'taxis': 17515, 'ride': 17516, 'chhattisgarhâ€¦': 17517, 'resistance': 17518, 'narration': 17519, 'offenders': 17520, 'penal': 17521, 'flexible': 17522, 'anyoneâ€': 17523, 'bourbon': 17524, 'killers': 17525, 'battery': 17526, 'andrÃ©s': 17527, 'fabiÃ¡n': 17528, 'hurtado': 17529, 'ibaguÃ©': 17530, '712': 17531, 'tier': 17532, 'deniers': 17533, '431823': 17534, '1900': 17535, 'devoto': 17536, 'zealand\\u2063\\u2063\\u2063\\u2063': 17537, 'case\\u2063\\u2063\\u2063': 17538, '2476': 17539, '465066': 17540, 'manouevres': 17541, 'wargames': 17542, 'f1': 17543, 'â€œcouldnâ€™t': 17544, 'helmets': 17545, '1387': 17546, '7219': 17547, '758027': 17548, 'graphics': 17549, 'â€œministry': 17550, 'khurana': 17551, 'generalcentral': 17552, 'homoeopathy': 17553, 'phdcci': 17554, '311k': 17555, 'professions': 17556, 'avenge': 17557, 'psychics': 17558, 'cto': 17559, 'Â£35bn': 17560, 'fraudsters': 17561, '73998': 17562, '2265': 17563, 'genomemodifying': 17564, '5360': 17565, '7292': 17566, '2810': 17567, 'burial': 17568, 'theprints': 17569, 'catalyse': 17570, 'medicalgrade': 17571, '641400': 17572, '86224': 17573, '1987955': 17574, '65803': 17575, 'gatesâ€™': 17576, 'â€œultimate': 17577, 'â€œmicrochip': 17578, 'â€œvirtual': 17579, 'idsâ€': 17580, 'â€œto': 17581, 'â€œfakeâ€': 17582, '102k': 17583, '1k': 17584, '1438': 17585, 'texasâ€”1826â€”in': 17586, '74000': 17587, 'lemons': 17588, '26k': 17589, 'bloggers': 17590, 'guidelinesâ€¦': 17591, 'marlin': 17592, 'stricken': 17593, 'coronapalooza': 17594, 'floridamarlins': 17595, 'weeklong': 17596, 'awesome': 17597, 'pbs': 17598, 'retail\\u200b': 17599, 'encouragement': 17600, 'engagement\\u200b': 17601, 'prosecution': 17602, 'indispensable': 17603, 'kolar': 17604, 'â‚¹25lak': 17605, 'representativesmpmlamlcssimply': 17606, '15lacm': 17607, 'clâ€¦': 17608, 'numerical': 17609, 'infectiousness': 17610, 'postrecovery': 17611, 'ramzan': 17612, '4113811': 17613, 'inescapable': 17614, 'slat': 17615, 'directorate': 17616, '60565728': 17617, '1136613': 17618, 'â€œhalf': 17619, 'idledâ€': 17620, 'favourable': 17621, 'universityastrazeneca': 17622, '20001120000': 17623, 'yaba': 17624, 'paralyse': 17625, '496000': 17626, '18852': 17627, 'badalkarapnavyavaharkareincoronaparvaar': 17628, 'coughingsneezing': 17629, 'handshakes': 17630, 'psychic': 17631, 'kaanipakam': 17632, 'quartaine': 17633, 'collectorthe': 17634, 'roming': 17635, 'templewill': 17636, 'chitoor': 17637, 'masjids': 17638, 'etcwhat': 17639, 'undertested': 17640, 'dismay': 17641, 'poledance': 17642, 'lapdance': 17643, 'plateau60': 17644, 'lagos17': 17645, '53021': 17646, '40281': 17647, '116k': 17648, '60s': 17649, 'repatriation': 17650, '64392594': 17651, '731534': 17652, 'betray': 17653, 'honorably': 17654, 'checkovir': 17655, 'saver': 17656, 'rainy': 17657, 'logical': 17658, '3k': 17659, 'proliferation': 17660, '81lagos': 17661, '26borno': 17662, '26kano': 17663, '20bauchi': 17664, '13fct': 17665, '12edo': 17666, '10sokoto': 17667, '7zamfara': 17668, '2ekiti': 17669, '4399': 17670, 'maskless': 17671, 'athletic': 17672, 'naukri': 17673, 'chheeni': 17674, 'dimaag': 17675, '23600': 17676, '3302': 17677, '259152': 17678, 'purdue': 17679, 'slbp': 17680, 'checklist': 17681, 'workflow': 17682, 'microbiological': 17683, '485k': 17684, 'covid19outbreak': 17685, 'vogelcheck': 17686, 'nonemergency': 17687, 'riphumanity': 17688, 'revenge': 17689, 'karma': 17690, 'melb': 17691, '479': 17692, 'modas': 17693, 'operandi': 17694, 'unclaimed': 17695, 'hira': 17696, 'hanna': 17697, 'newmom': 17698, 'farce': 17699, 'â€œlysol': 17700, 'killsâ€': 17701, '1220': 17702, '312': 17703, '2314': 17704, '4807': 17705, '426': 17706, 'ifcn': 17707, '4031': 17708, '3965': 17709, '271': 17710, '24500': 17711, 'jugular': 17712, 'geolocate': 17713, '1980s': 17714, 'knack': 17715, 'remake': 17716, 'sharona': 17717, 'theknack': 17718, 'mitigationâ€': 17719, '2172': 17720, '181': 17721, 'gallery': 17722, 'arts': 17723, '483': 17724, '15lakhaagaye': 17725, 'failedmodi': 17726, 'modigovernment': 17727, '648': 17728, 'plateau148': 17729, 'ondo42': 17730, 'kwara38': 17731, '41180': 17732, '18203': 17733, '860': 17734, 'practiceâ€¦': 17735, 'amass': 17736, 'brescia': 17737, '770': 17738, 'allahabad': 17739, 'hc': 17740, 'saket': 17741, 'gokhale': 17742, 'activistâ€™s': 17743, 'restrain': 17744, 'bhoomi': 17745, 'poojan': 17746, '1115am': 17747, '664k': 17748, 'webbased': 17749, '44indias': 17750, 'arsenal': 17751, 'beppe': 17752, 'grillo': 17753, 'teamsurveillance': 17754, 'issuance': 17755, 'ann': 17756, 'coulter': 17757, 'incapacitate': 17758, 'tuberulosis': 17759, '2985': 17760, '1407271': 17761, 'thosâ€¦': 17762, '1304': 17763, '18091': 17764, '657506': 17765, 'sedatives': 17766, 'analgesics': 17767, 'intubation': 17768, 'â€˜covid19': 17769, 'differentiate': 17770, 'selfreport': 17771, 'symptomsâ€™': 17772, 'preventcovid19spread': 17773, 'facilitiessanitisers': 17774, 'dish': 17775, 'towel': 17776, 'linens': 17777, 'highlevelâ€¦â€': 17778, 'silvina': 17779, 'masciotra': 17780, 'submission': 17781, 'hepatitisb': 17782, 'vacâ€¦': 17783, 'answerâ€¦': 17784, 'extremeheat': 17785, 'heatrelated': 17786, 'batâ€¦': 17787, 'prepareâ€¦': 17788, 'wawona': 17789, 'seasonâ€¦': 17790, 'ws': 17791, 'wearâ€¦': 17792, 'independentâ€¦': 17793, 'â€œgo': 17794, 'kitâ€': 17795, '9600': 17796, '187000': 17797, 'stis': 17798, 'unintended': 17799, 'interactions': 17800, 'sanitation': 17801, 'adjuvanted': 17802, 'fluvaccines': 17803, 'quadrivalent': 17804, '18002221222': 17805, '797979': 17806, 'froâ€¦': 17807, 'geographically': 17808, 'dispersâ€¦': 17809, 'bloodplasma': 17810, 'saveâ€¦': 17811, '6âƒ£ft': 17812, 'crowâ€¦': 17813, 'famâ€¦': 17814, 'haâ€¦': 17815, 'wawonabrand': 17816, 'cdcyrbs': 17817, 'pharmacists': 17818, 'outreach': 17819, 'â€œ20202021': 17820, 'uncertainties': 17821, 'afmawareness': 17822, 'bâ€¦': 17823, 'oralhealth': 17824, 'acrosâ€¦': 17825, 'tele': 17826, 'opioids': 17827, 'otâ€¦': 17828, 'brochures': 17829, 'ins': 17830, 'spaz': 17831, '99xx': 17832, 'takapuna': 17833, 'afrorc70': 17834, 'themselvesitâ€™s': 17835, 'stayathomekind': 17836, 'nycâ€': 17837, '4648': 17838, '2482': 17839, '3589': 17840, '2421': 17841, 'effectiâ€¦': 17842, 'snoop': 17843, '1772': 17844, 'familiar': 17845, '4465863': 17846, '777': 17847, 'unrelated': 17848, '11010': 17849, '730330': 17850, 'aircraft': 17851, 'fume': 17852, 'raissa': 17853, 'rui': 17854, 'bahias': 17855, 'becaused': 17856, 'favour': 17857, 'additioâ€¦': 17858, 'â€œthese': 17859, 'americansâ€™': 17860, 'ghanas': 17861, '1639599': 17862, '643948': 17863, '2764': 17864, 'stupidity': 17865, 'loyalists': 17866, 'minimal': 17867, 'sicken': 17868, 'fuller': 17869, 'stripperâ€™s': 17870, 'infest': 17871, 'covid19â€“positive': 17872, 'awardwinning': 17873, 'elder': 17874, 'wes': 17875, 'studi': 17876, 'â€œtsiichin': 17877, 'bii': 17878, 'tÃ³â€': 17879, '32k': 17880, '600800': 17881, 'dayâ€¦': 17882, 'â€œdoctors': 17883, 'preparedâ€': 17884, 'ovidâ€™': 17885, 'â€˜seeâ€™': 17886, 'â€œnumber': 17887, 'surrenderâ€': 17888, 'timesâ€': 17889, 'conclusion': 17890, 'â€˜see': 17891, 'breadmaking': 17892, 'apartment': 17893, '3th6th': 17894, 'robertson': 17895, 'ladies': 17896, 'wiâ€¦': 17897, 'asias': 17898, 'mukesh': 17899, 'ambanis': 17900, 'nita': 17901, 'ambani': 17902, 'rs500': 17903, 'goa': 17904, 'argument': 17905, 'sow': 17906, 'invisibility': 17907, 'isotypes': 17908, 'igm': 17909, '176lagos': 17910, '65kano': 17911, '31katsina': 17912, '20fct': 17913, '14nasarawa': 17914, '10plateau': 17915, '4rivers': 17916, '679': 17917, 'wastewater': 17918, 'peoplecentric': 17919, 'exponentiaâ€¦': 17920, '868042': 17921, 'apparent': 17922, 'everyonewear': 17923, 'outsidei': 17924, 'wrongbut': 17925, 'youthe': 17926, 'youim': 17927, 'smartbut': 17928, 'andorra': 17929, '21020': 17930, 'quantity': 17931, 'reusable': 17932, 'lighter': 17933, 'remedesivir': 17934, '366k': 17935, 'mart': 17936, 'rani': 17937, 'checkpoints': 17938, 'areaâ€”while': 17939, 'stayhomeforthem': 17940, 'javier': 17941, 'â€œhoaxâ€': 17942, 'mueller': 17943, 'oppenings': 17944, 'ifema': 17945, 'outta': 17946, 'â€˜certificate': 17947, 'â€œtrump': 17948, 'earlywarning': 17949, 'coronavirusesâ€': 17950, '7100': 17951, '218000': 17952, 'prey': 17953, 'iceland': 17954, '55624': 17955, '33748': 17956, 'aidsfree': 17957, 'respoâ€¦': 17958, 'salariespensions': 17959, 'deduct': 17960, '274': 17961, 'irans': 17962, 'ayatollah': 17963, 'khamenei': 17964, 'poorest': 17965, 'marginalise': 17966, 'worldmalariaday': 17967, 'edt': 17968, 'globalâ€¦': 17969, 'newlyimposed': 17970, 'critâ€¦': 17971, 'testingâ€¦': 17972, '65325779': 17973, '933185': 17974, 'someoneâ€™s': 17975, 'accomplishedâ€': 17976, 'rusty': 17977, 'enscore': 17978, 'inventory': 17979, 'tons': 17980, 'eho': 17981, '13498': 17982, 'atest': 17983, 'departmentâ€™s': 17984, 'incriminate': 17985, 'revelations': 17986, 'unravel': 17987, 'corroboration': 17988, 'â€œconfidentâ€': 17989, 'sumatra': 17990, 'abhealthandwellnesscentres': 17991, 'â€œstay': 17992, 'homeâ€': 17993, '586244': 17994, '586298': 17995, 'lengthen': 17996, 'cliff': 17997, 'â€œcoronavirusinfestedâ€': 17998, 'cunts': 17999, 'meâ€': 18000, 'moan': 18001, 'georgiapeaches': 18002, '900k': 18003, 'purposely': 18004, 'screw': 18005, 'fault': 18006, 'travelhes': 18007, 'virusits': 18008, 'infusions': 18009, 'unlock4â€¦': 18010, 'aotearoa': 18011, 'teamof5million': 18012, 'exposures': 18013, 'ifain': 18014, 'yougov': 18015, '33k': 18016, 'thenâ€”but': 18017, '5942': 18018, '840': 18019, 'nonpharmaceuticals': 18020, 'citizensâ€': 18021, 'sticky': 18022, 'coughsneeze': 18023, 'sue': 18024, '77000': 18025, 'revelry': 18026, 'registration': 18027, 'conjunctivitis': 18028, '167k': 18029, '2ndlargest': 18030, 'lula': 18031, 'monster': 18032, 'nationalisâ€¦': 18033, 'ambition': 18034, 'superpower': 18035, '552': 18036, 'shopkeepers': 18037, 'testkits': 18038, 'raisin': 18039, 'mercilessly': 18040, 'sixteen': 18041, 'lame': 18042, 'concept': 18043, '5950': 18044, '341117': 18045, '61000': 18046, 'temples': 18047, 'dargahs': 18048, 'donkey': 18049, 'penises': 18050, 'aeroplan': 18051, 'worldsteamweek': 18052, '1406': 18053, 'â€œitalian': 18054, '54859': 18055, 'wisconsinites': 18056, 'gurus': 18057, 'underreported': 18058, 'â€œmore': 18059, 'patrolâ€': 18060, 'river1': 18061, '54588': 18062, '42627': 18063, '1048': 18064, 'pinarayi': 18065, 'vijayan': 18066, 'annals': 18067, 'falsenegative': 18068, '279485': 18069, '54134': 18070, 'vele': 18071, 'markovski': 18072, 'claimd': 18073, 'contamination': 18074, 'sarcov2': 18075, 'forth': 18076, 'gorilla': 18077, 'nuclear': 18078, 'â€œdoes': 18079, 'seeâ€': 18080, 'â€œkeep': 18081, 'bagong': 18082, 'alyansang': 18083, 'makabayan': 18084, 'bayan': 18085, 'renato': 18086, 'reyes': 18087, 'sr': 18088, '2099': 18089, '962': 18090, 'programâ€': 18091, 'enablementdrive': 18092, '1855745': 18093, '1230509': 18094, 'phase3': 18095, 'homegrown': 18096, 'whatâ€¦': 18097, 'plateau116': 18098, 'ekiti12': 18099, 'ebonyi8': 18100, 'benue7': 18101, '54247': 18102, '42010': 18103, 'â€œoh': 18104, 'unhealthyâ€': 18105, 'smug': 18106, 'ghost': 18107, 'upton': 18108, 'ers2020': 18109, 'largeâ€”12700â€“and': 18110, '185092': 18111, 'ncdcfactcheck': 18112, 'cigarettes': 18113, 'â€œtrumpâ€': 18114, 'â€œpenceâ€': 18115, 'moro': 18116, 'informal': 18117, 'worldhealthdata': 18118, 'convergebio': 18119, 'togetherletsbringthechange': 18120, '69000': 18121, 'gajendra': 18122, 'shekhawat': 18123, 'maranhÃ£o': 18124, 'anxious': 18125, 'lifeline': 18126, '8002738255': 18127, 'crucify': 18128, 'queenelizabethii': 18129, 'princeharry': 18130, 'miley': 18131, 'cyrus': 18132, 'vagina': 18133, 'clue': 18134, 'lmao': 18135, 'unplanned': 18136, '1834': 18137, 'eastâ€': 18138, 'savinglives': 18139, '129000': 18140, 'fmc': 18141, 'keffi': 18142, 'biomolecular': 18143, '100m': 18144, 'schoolrelated': 18145, 'â€˜north': 18146, 'koreanâ€™': 18147, 'anarchy': 18148, 'methodology': 18149, 'underprepared': 18150, '6900': 18151, 'tonne': 18152, '2800': 18153, 'tonneday': 18154, '233843\\u2063\\u2063': 18155, '5500': 18156, 'mastheads': 18157, 'spoiler': 18158, '55005': 18159, '43013': 18160, 'chattisgarh': 18161, '7899': 18162, '8085': 18163, 'ladahk': 18164, '8673': 18165, 'sonipat': 18166, '816': 18167, '3865': 18168, '194191': 18169, '144k': 18170, '667k': 18171, 'trustworthy': 18172, 'unitedkingdom': 18173, '417k': 18174, 'persistence': 18175, 'postviral': 18176, 'uncoordinated': 18177, 'bungle': 18178, 'defecate': 18179, 'saharanpur': 18180, 'nokia': 18181, 'passto': 18182, 'porsche': 18183, 'carwo': 18184, 'azar': 18185, 'ind': 18186, '1053': 18187, '296000\\u2063': 18188, 'atmanirbharbharat': 18189, 'jellyfish': 18190, 'whÄnau': 18191, 'zimbabwe': 18192, 'us60': 18193, 'sectors': 18194, 'incountryâ€': 18195, 'errands': 18196, 'giammattei': 18197, 'han': 18198, 'hydroxycholoroquine': 18199, 'newscorp': 18200, '790': 18201, 'delta166': 18202, 'lagos120': 18203, 'enugu66': 18204, 'ogun43': 18205, 'kano41': 18206, 'ondo33': 18207, 'bayelsa29': 18208, 'imo20': 18209, '26484': 18210, '10152': 18211, 'quarterfinals': 18212, '19522': 18213, '2231': 18214, '1224': 18215, '551': 18216, 'â€œsame': 18217, 'fakenewsâ€': 18218, 'leaflets': 18219, 'abhigya': 18220, 'â€˜thousands': 18221, 'vaccineâ€™': 18222, '21411': 18223, 'atheist': 18224, 'christianity': 18225, 'technologists': 18226, 'wholeofsociety': 18227, '033': 18228, '17720206': 18229, 'steeply': 18230, 'bolster': 18231, 'psychosocial': 18232, 'restoration': 18233, 'lebanese': 18234, 'beirut': 18235, '1132': 18236, 'mâ€¦': 18237, 'devolve': 18238, '277': 18239, 'centuries': 18240, 'â€˜independent': 18241, 'adviserâ€™': 18242, 'uzbekistan': 18243, 'genomically': 18244, 'â€œasymptomaticâ€': 18245, 'â€œpresymptomaticâ€': 18246, 'unsustainable': 18247, 'slash': 18248, 'Â£70bn': 18249, 'influential': 18250, 'digitize': 18251, 'useassociated': 18252, 'evali': 18253, '12â€“apr': 18254, 'lagos117': 18255, 'osun24': 18256, 'ebonyi18': 18257, 'delta17': 18258, 'anambra14': 18259, '47290': 18260, '33609': 18261, 'infront': 18262, '76lagos': 18263, '37katsina': 18264, '32jigawa': 18265, '23kano': 18266, '19fct': 18267, '18borno': 18268, '10edo': 18269, '6adamawa': 18270, '2802': 18271, '048831145': 18272, '1092654': 18273, 'doublecheck': 18274, 'reportingâ€”our': 18275, 'tremendously': 18276, 'maximal': 18277, 'affirm': 18278, 'maroole': 18279, 'forum': 18280, 'oromo': 18281, 'icmrdelhi': 18282, 'arterial': 18283, 'venous': 18284, 'anticoagulation': 18285, '1050': 18286, 'coronavirusnyc': 18287, 'capitol': 18288, 'roadside': 18289, 'bathroom': 18290, 'blackmagic': 18291, '619508': 18292, '1048971': 18293, 'redisse': 18294, 'aloud': 18295, 'idph': 18296, 'getz': 18297, 'bethe1to': 18298, 'suicideprevention': 18299, 'zoran': 18300, 'zaev': 18301, 'refuge': 18302, 'wouldbe': 18303, 'robbers': 18304, 'minors': 18305, 'consciousness': 18306, 'receptors': 18307, '3089': 18308, '422144': 18309, 'cadence': 18310, 'bronwyn': 18311, '14days': 18312, 'element': 18313, 'splash': 18314, 'hardearned': 18315, 'hostage': 18316, 'dÃ©nis': 18317, 'oblige': 18318, '745k': 18319, '46k': 18320, '1024': 18321, 'dubliner': 18322, 'janey': 18323, 'mac': 18324, 'lovely': 18325, '16\\u200b': 18326, 'lowtomoderate': 18327, 'umraniye': 18328, 'fct3': 18329, '56256': 18330, '44152': 18331, '1082': 18332, 'empressmarket': 18333, 'ngo': 18334, 'â€”not': 18335, 'totalâ€”but': 18336, 'jeremy': 18337, 'corbyn': 18338, 'bastard': 18339, 'recentlyidentified': 18340, 'stylists': 18341, 'mayorâ€™s': 18342, '4day': 18343, 'wean': 18344, 'altogether': 18345, '65050': 18346, '7660': 18347, 'wealth': 18348, 'pathogen': 18349, 'alledgely': 18350, '2654': 18351, 'kâ€¦': 18352, 'rational': 18353, 'beech': 18354, 'cbseâ€™s': 18355, 'oreilly': 18356, 'kenyatta': 18357, 'bureaus': 18358, 'delist': 18359, 'loan': 18360, 'taxâ€': 18361, 'bernardino': 18362, 'juneâ€¦': 18363, 'â€œreal': 18364, 'chancesâ€': 18365, 'influencers': 18366, '3908': 18367, 'gulags': 18368, '515': 18369, 'muskâ€™s': 18370, 'endorsements': 18371, 'openly': 18372, 'childrenâ€™s': 18373, 'mishandle': 18374, 'â€œiâ€™m': 18375, 'adoptionâ€': 18376, 'irelandâ€™s': 18377, 'swedenâ€˜s': 18378, 'itâ€˜s': 18379, '16399': 18380, '1904': 18381, '294848': 18382, 'weekends\\u2063': 18383, 'klain': 18384, 'lectin': 18385, 'anticovid19': 18386, 'authorise': 18387, '850000': 18388, 'paradigm': 18389, 'â€œshines': 18390, 'lighton': 18391, 'inpatient': 18392, 'specificity': 18393, 'milkthe': 18394, 'risâ€¦': 18395, '15001115000': 18396, '12midnight': 18397, 'onwards': 18398, 'plateau85': 18399, 'enugu46': 18400, 'ibom7': 18401, '53317': 18402, '40726': 18403, '1521': 18404, '32million': 18405, 'errrris': 18406, 'nowhere': 18407, 'feku': 18408, 'unroll': 18409, 'areâ€¦': 18410, 'echr': 18411, 'antoine': 18412, 'buyse': 18413, 'corkonians': 18414, 'cork': 18415, 'notcork': 18416, 'willie': 18417, 'willied': 18418, 'hiphop': 18419, 'dontvacinateyourself': 18420, 'spacescovid': 18421, 'widethere': 18422, 'â€˜s': 18423, 'roadmapoutoflockdown': 18424, 'lockdowneasing': 18425, 'largestvaccinedrive': 18426, 'covid19ab': 18427, '161736': 18428, '171058': 18429, 'beside': 18430, 'vaccineforyoungistaan': 18431, 'covidvaccines': 18432, 'vaccineshortage': 18433, 'â€œ': 18434, 'har': 18435, 'aise': 18436, 'jaise': 18437, 'aakhri': 18438, 'hoâ€': 18439, 'cbseboardexams2021': 18440, 'nuremberg': 18441, 'worthless': 18442, 'novaccinepassportsanywhere': 18443, 'novaccinepassport': 18444, 'teamvaccines': 18445, 'nit': 18446, 'calicut': 18447, 'gotta': 18448, 'hostel': 18449, 'rodent': 18450, 'pixie': 18451, 'elf': 18452, 'immunobiology': 18453, 'interleukins': 18454, 'washyourhands': 18455, 'pathetically': 18456, 'silence': 18457, 'deafen': 18458, '9pm': 18459, 'thursdays': 18460, 'belfasthour': 18461, 'smes': 18462, 'edwardsandco': 18463, 'Î²1': 18464, 'integrins': 18465, 'reovirus': 18466, 'cancerresearch': 18467, 'seminar': 18468, 'bergeijk': 18469, 'globalisation': 18470, 'â²300': 18471, 'cet': 18472, 'kenney': 18473, 'abpoli': 18474, 'yeg': 18475, 'nifty': 18476, 'sensex': 18477, 'banknifty': 18478, 'onlineexams': 18479, 'governance': 18480, 'governancefailure': 18481, 'mahakumbh2021': 18482, 'bengalelection2021': 18483, 'mutant': 18484, 'maskupindia': 18485, 'tonights': 18486, 'sneer': 18487, 'cohesion': 18488, 'digitization': 18489, 'coauthor': 18490, 'positionpaper': 18491, 'pto': 18492, 'staycation': 18493, 'shamelessselfpromo': 18494, 'indieapril': 18495, 'covidfiction': 18496, 'unprecedentedtimes': 18497, 'khalsa': 18498, 'prominent': 18499, 'charities': 18500, 'sikh': 18501, 'â€˜treating': 18502, '9newsaus': 18503, 'tga': 18504, '9news': 18505, 'chooseday': 18506, 'tuesdayvibe': 18507, 'tuesdaymotivations': 18508, 'justiceforlufuno': 18509, 'jackiechan': 18510, 'pretoria': 18511, 'zulu': 18512, 'johannesburg': 18513, 'translation': 18514, 'translator': 18515, 'kzn': 18516, 'ranchicity': 18517, 'orchidranchi': 18518, 'non': 18519, 'paitent': 18520, 'laygude': 18521, 'sinhgad': 18522, 'maharashtraneedsvaccine': 18523, 'thong': 18524, 'lor': 18525, 'thonglor': 18526, 'entertainmentvenue': 18527, 'thailandnews': 18528, 'rlf': 18529, 'novacyt': 18530, 'whistler': 18531, 'ski': 18532, '877': 18533, 'variant': 18534}\n"
     ]
    }
   ],
   "source": [
    "print('The total number of words: ', len(tokenizer.word_index) + 1)\n",
    "print(tokenizer.word_index)"
   ]
  }
 ]
}