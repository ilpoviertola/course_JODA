{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0a6d4ddde8f020d22769858c6343faf50cf54e29c922f4da84f6f66e8323f4169",
   "display_name": "Python 3.7.10 64-bit ('JODA': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/ilpoviertola/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/ilpoviertola/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                              tweet label\n",
       "0   1  The CDC currently reports 99031 deaths. In gen...  real\n",
       "1   2  States reported 1121 deaths a small rise from ...  real\n",
       "2   3  Politically Correct Woman (Almost) Uses Pandem...  fake\n",
       "3   4  #IndiaFightsCorona: We have 1524 #COVID testin...  real\n",
       "4   5  Populous states can generate large case counts...  real"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>The CDC currently reports 99031 deaths. In gen...</td>\n      <td>real</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>States reported 1121 deaths a small rise from ...</td>\n      <td>real</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n      <td>fake</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n      <td>real</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Populous states can generate large case counts...</td>\n      <td>real</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "# Download nltk-packages (not downloaded if up to date)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import os\n",
    "\n",
    "\n",
    "# Load the training dataset to dataframe.\n",
    "dataset_path = '/Users/ilpoviertola/OneDrive - TUNI.fi/Kurssimateriaaleja/JODA/datasets/covid19_fake_news'\n",
    "train_df = pd.read_csv(dataset_path+'/Constraint_train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Datatypes: \nid        int64\ntweet    object\nlabel    object\ndtype: object\n\nAmount of NaN-values = 0\n\nIs id unique? = True\n"
     ]
    }
   ],
   "source": [
    "# Examine training dataset.\n",
    "\n",
    "print('Datatypes: \\n' + str(train_df.dtypes) + '\\n')\n",
    "print('Amount of NaN-values = ' + str(train_df.isna().sum().sum()) + '\\n')\n",
    "print('Is id unique? = ' + str(train_df['id'].is_unique))"
   ]
  },
  {
   "source": [
    "Everything seems to be ok. No none-values and every row has unique id. Let's alter the training dataset a bit now!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Real (1) and fake (0) news amounts in training dataset:\n1    3360\n0    3060\nName: real, dtype: int64\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               tweet  real\n",
       "0  The CDC currently reports 99031 deaths. In gen...     1\n",
       "1  States reported 1121 deaths a small rise from ...     1\n",
       "2  Politically Correct Woman (Almost) Uses Pandem...     0\n",
       "3  #IndiaFightsCorona: We have 1524 #COVID testin...     1\n",
       "4  Populous states can generate large case counts...     1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>real</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The CDC currently reports 99031 deaths. In gen...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>States reported 1121 deaths a small rise from ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Populous states can generate large case counts...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# Alter training dataset a bit.\n",
    "train_df_copy = train_df.copy()\n",
    "\n",
    "# Change label column to real where 1 indicates real news and 0 false news.\n",
    "dummy = pd.get_dummies(train_df['label'])\n",
    "train_df_copy = pd.concat([train_df_copy, dummy], axis=1)\n",
    "train_df_copy = train_df_copy.drop(['fake', 'label', 'id'], axis=1)\n",
    "print(\"Real (1) and fake (0) news amounts in training dataset:\")\n",
    "print(train_df_copy['real'].value_counts())\n",
    "\n",
    "# Split dataframe into two lists containing tweets (x_train) and the real value (y_train).\n",
    "x_train = train_df_copy['tweet'].tolist()\n",
    "y_train = train_df_copy['real'].tolist()\n",
    "\n",
    "train_df_copy.head()"
   ]
  },
  {
   "source": [
    "No we have changed the old 'label'-colum to 'real' and binary coded its values. This means that if the given tweet is real news, 'real'-columns value is 1. If the tweet is fake news this value is 0. We also split the dataframe into two lists: x_train and y_train. x_train contains the tweets in the same order that y_train contains the real/fake value."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6420, 16267)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# Preprocess text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenise words\n",
    "    tokeniser = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokeniser.tokenize(text)\n",
    "\n",
    "    # Lowercase and lemmatise (e.g. Driving -> drive)\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]\n",
    "\n",
    "    # Remove stopwords\n",
    "    keywords = [lemma for lemma in lemmas if lemma not in stopwords.words('english')]\n",
    "\n",
    "    return keywords\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer=preprocess_text)\n",
    "train_df_tfidf = vectorizer.fit_transform(x_train)\n",
    "train_df_tfidf.shape"
   ]
  },
  {
   "source": [
    "We will used TfidfVectorizer here to convert our tweets into Term Frquency - Inverse Document Frequency -feature matrix. Short explanation about TF-IDF:   \n",
    "Words that are common in every tweet (such as what, when, but,...) rank low even though they may appear many times, since they don’t mean much to that document in particular.  \n",
    "However, if the word 'hoax' appears many times in a document, while not appearing many times in others, it probably means that it’s very relevant in our case.  \n",
    "\n",
    "preprocess_text(text) function takes a tweet and preprocesses it:   \n",
    "1. Tokenization: Tokens (in our case unique words) are smaller pieces of text, which are used as features of a tweet.  \n",
    "2. Lowercasing and lemmatization: Every word will be lowercased, so You and you will not be two features in our feature matrix. Lemmatization aims to return the base or dictionary form of a word, which is known as the lemma.  \n",
    "3. Stopwords: A stopword is a commonly used word like 'a', 'an' or 'the'. These words are not so important to our classification process so they can be removed.\n",
    "   \n",
    "train_df_tfidf is now a feature matrix where:\n",
    "- amount of rows = amount of tweets in the dataset\n",
    "- amount of columns = amount of features aka. unique words in the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.94781931 0.93302181 0.94548287 0.9470405  0.93613707]\nAccuracy: 0.94 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "sgd_clf = SGDClassifier(random_state=123)\n",
    "sgf_clf_scores = cross_val_score(sgd_clf, train_df_tfidf, y_train, cv=5, scoring='accuracy')\n",
    "print(sgf_clf_scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (sgf_clf_scores.mean(), sgf_clf_scores.std() * 2))"
   ]
  },
  {
   "source": [
    "Used model is SGDClassifier, which is linear classifier that uses sthocastic gradient descent learning. This works well with our sparse-matrix (feature-matrix)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'early_stopping': False, 'fit_intercept': True, 'loss': 'hinge', 'penalty': 'l2'}\n[0.94781931 0.93302181 0.94548287 0.9470405  0.93613707]\nAccuracy: 0.94 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "# Improve the model & run it with new params\n",
    "grid = {'fit_intercept': [True,False],\n",
    "        'early_stopping': [True, False],\n",
    "        'loss' : ['hinge', 'log', 'squared_hinge'],\n",
    "        'penalty' : ['l2', 'l1', 'none']}\n",
    "search = GridSearchCV(estimator=sgd_clf, param_grid=grid, cv=5)\n",
    "search.fit(train_df_tfidf, y_train)\n",
    "print(search.best_params_)\n",
    "\n",
    "grid_sgd_clf_scores = cross_val_score(search.best_estimator_, train_df_tfidf, y_train, cv=5)\n",
    "print(grid_sgd_clf_scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (grid_sgd_clf_scores.mean(), grid_sgd_clf_scores.std() * 2))\n"
   ]
  },
  {
   "source": [
    "After computing the baseline-model, we can fine tune it with GridsearchCV. GridsearchCV performs and exhaustive search over specified parameter values for an estimator. E.g. we give a bunch of different loss-functions for GridsearchCV and it returns the one that generates a best results.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectoriser',\n",
       "                 TfidfVectorizer(analyzer=<function preprocess_text at 0x7fc857d9dcb0>)),\n",
       "                ('classifier', SGDClassifier(random_state=123))])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "pipe = Pipeline([('vectoriser', vectorizer),\n",
    "                 ('classifier', search.best_estimator_)])\n",
    "pipe.fit(x_train, y_train)"
   ]
  },
  {
   "source": [
    "After fine tuning the model, we can create a pipeline which integrates the creation of feature matrix (data transforamtion) and model into a single pipeline.  \n",
    "The pipeline first transforms the unstructured data to a feature matrix and then fits the preprocessed data to the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.94\n[[ 944   76]\n [  60 1060]]\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "test_df = pd.read_csv(dataset_path+'/Constraint_Val.csv')\n",
    "x_test = test_df['tweet'].tolist()\n",
    "test_df = pd.concat([test_df, pd.get_dummies(test_df['label'])], axis=1)\n",
    "test_df = test_df.drop(['fake', 'label', 'id'], axis=1)\n",
    "y_test = test_df['real'].tolist()\n",
    "\n",
    "y_test_pred = pipe.predict(x_test)\n",
    "print(\"Accuracy: %0.2f\" % (accuracy_score(y_test, y_test_pred)))\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "source": [
    "Now we have introduced our model to a dataset that it has not seen before! We can use the accuracy_score-function to determine how correct our model was. We predict the result (fake/real) for the input (tweet) and then compare these results to the real classes of those tweets in order to obtain the model's accuracy.  \n",
    "The confusion matrix below the accuracy value tells us how many ACTUAL reals our model PREDICTED as fakes and other way around. Below is illustration:  \n",
    "  \n",
    "|               | PRED. FAKE    | PRED. REAL  |\n",
    "| ------------- |:-------------:| :----------:|\n",
    "| ACTUAL FAKE   | \\#            | \\#          |\n",
    "| ACTUAL REAL   | \\#            | \\#          |\n",
    "\n",
    "## Now, let's get data from Twitter!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_key = ''\n",
    "access_key_secret = ''\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_key_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet_df = pd.DataFrame(columns=['username', 'description', 'location', 'following',\n",
    "                               'followers', 'totaltweets', 'retweetcount', 'text', 'hashtags'])\n",
    "tweets = tweepy.Cursor(api.search, q='#covid19', lang='en', since='2021-03-01', tweet_mode='extended').items(10)\n",
    "tweets_list = [tweet for tweet in tweets]\n",
    "for tweet in tweets_list:\n",
    "    username = tweet.user.screen_name\n",
    "    description = tweet.user.description\n",
    "    location = tweet.user.location\n",
    "    following = tweet.user.friends_count\n",
    "    followers = tweet.user.followers_count\n",
    "    totaltweets = tweet.user.statuses_count\n",
    "    retweetcount = tweet.retweet_count\n",
    "    hashtags = tweet.entities['hashtags']\n",
    "        \n",
    "    # Retweets can be distinguished by a retweeted_status attribute,\n",
    "    # in case it is an invalid reference, except block will be executed\n",
    "    try:\n",
    "        text = tweet.retweeted_status.full_text\n",
    "    except AttributeError:\n",
    "        text = tweet.full_text\n",
    "    hashtext = list()\n",
    "    for j in range(0, len(hashtags)):\n",
    "        hashtext.append(hashtags[j]['text'])\n",
    "        \n",
    "    # Here we are appending all the extracted information in the DataFrame\n",
    "    ith_tweet = [username, description, location, following,\n",
    "                    followers, totaltweets, retweetcount, text, hashtext]\n",
    "    tweet_df.loc[len(tweet_df)] = ith_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                                                                                                                                                                                                                                                                                 tweet  \\\n0  The evolution of Yan Limeng was carefully designed by Guo Wengui and Stephen K. Bannon. They fueled her deep-rooted belief that the virus is a product of genetic engineering, and she accepts the evidence she provides regardless of right or wrong.\\n#StevenBannon\\n#YanLiMeng\\n#COVID19 https://t.co/htj7jWczCw   \n1  The evolution of Yan Limeng was carefully designed by Guo Wengui and Stephen K. Bannon. They fueled her deep-rooted belief that the virus is a product of genetic engineering, and she accepts the evidence she provides regardless of right or wrong.\\n#StevenBannon\\n#YanLiMeng\\n#COVID19 https://t.co/htj7jWczCw   \n2  A quick remember that the #COVID19 comes from #China not UK don't let the #media manipulates you. #Chinamustpay https://t.co/evS3UxhrTk                                                                                                                                                                               \n3  Millions of women in the health and social sector work tirelessly every day to care for all of us. \\n\\nToday, we thank you. 👏 \\n#WorldHealthDay | #COVID19 | #coronavirus https://t.co/ivDq8kbcmG                                                                                                                     \n4  Before the outbreak of the new coronavirus, Yan Limeng had studied influenza, but had not studied the coronavirus. Like a clown, Yan Limeng was carefully designed by Chinese billionaire Guo Wengui and former adviser Stephen K. Bannon.\\n#StevenBannon\\n#YanLiMeng\\n#COVID19 https://t.co/IQqVDuRjA7               \n5  Michigan confirmed new hospitalizations for #COVID19 has soared an alarming +43% in just one week! \\n\\nIt’s now more than double the national US per 100 bed rate (12 vs 5). \\n\\nNursing home staff cases also up +8% in one week. Positivity up 4% points in one week. Not good. https://t.co/yfYpFrNJKP             \n6  Michigan confirmed new hospitalizations for #COVID19 has soared an alarming +43% in just one week! \\n\\nIt’s now more than double the national US per 100 bed rate (12 vs 5). \\n\\nNursing home staff cases also up +8% in one week. Positivity up 4% points in one week. Not good. https://t.co/yfYpFrNJKP             \n7  Kes positif mengikut negeri (setakat 07/04/2021, 12 pm)\\n\\nConfirmed cases by state (as of 07/04/2021, 12 pm)\\n\\n#COVID19\\n#KitaMestiMenang\\n#sihatmilikku https://t.co/1JULi3lCFd                                                                                                                                    \n8  Sorry, but the NACI has it wrong. Pfizer and Moderna were quite clear in their data about maximum efficacy for their vaccines. 2nd dose 21 days for Pfizer, 28 days for Moderna with a tolerable delay of 5-6 weeks - that's fourty days, NOT four months! @GovCanHealth #COVID19                                     \n9  We know #COVID19 has had a HUGE impact on health and human service agencies -- what are the biggest challenges they face? We surveyed agencies this fall to find out just that -- learn more here https://t.co/BfPmTu4Bxz https://t.co/qe7WMlk7Gq                                                                     \n\n   is real?  \n0  0         \n1  0         \n2  1         \n3  1         \n4  0         \n5  1         \n6  1         \n7  1         \n8  1         \n9  1         \n"
     ]
    }
   ],
   "source": [
    "tweet_text_list = tweet_df['text'].tolist()\n",
    "tweet_pred = pipe.predict(tweet_text_list)\n",
    "tweet_pred_df = pd.DataFrame({'tweet':tweet_text_list, 'is real?':tweet_pred})\n",
    "\n",
    "print(tweet_pred_df)"
   ]
  }
 ]
}