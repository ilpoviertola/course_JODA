# Harjoitustyö 3: Datan jalostaminen
### *Ilpo Viertola, Johdanto datatieteeseen 2021*  
  
Tässä harjoitustyöraportissa käydään lävitse, kuinka dataa jalostettiin ja twiiteistä eroteltiin piirteet. Tätä dataa tullaan myöhemmin käyttämään neuroverkossa, jonka tekemiseen hyödynnetään Pythonin Keras-kirjastoa. Valittu koneoppimisen menetelmä vaikuttaa vahvasti siihen, miten dataa tullaan käsittelemään. 

## Datan siivoaminen
  
Lähtökohtana oli neljä Pandas DataFramea, jotka sisälsivät käsittelemätöntä dataa. Kolme näistä sisälsivät Kagglesta haettua dataa ja yksi Twitter API:n kautta kerättyjä #covid19-tagillä varustettuja twiittejä. Kagglen datasetteihin lukeutuu mallin opetukseen, validointiin ja testaukseen kätetyt datasetit, jotka sisältävät covid19-aiheisia sosiaalisen median julkaisuja. Kaikille julkaisuille on myös määritetty totuusarvo, onko tekstin uutinen/väittämä totta vai tarua. Mallin toimivuutta reaalimaailmassa on tarkoitus testata Twitter API:lla kerätyn datasetin avulla.
  
Aivan ensimmäisenä tarkastetaan, sisältävätkö datasetit Null-arvoja ja onko kolumnien datatyypit kunnossa. Alla muutamia koodipätkiä, joilla työ on tehty. En ole tähän dokumenttiin kirjannut operaatioita kuin kahdelle datasetille, koska ne ovat samankaltaisia myös muille.

```python
print('Null values in training data? ' + str(train_df.isnull().values.any()))
print('Null values in Twitter data? ' + str(tweet_df.isnull().values.any()))
```
    Null values in training data? False
    Null values in Twitter data? False

```python
print('Datatypes for training data: \n' + str(train_df.dtypes) + '\n')
print('Datatypes for Twitter data: \n' + str(tweet_df.dtypes) + '\n')
```

    Datatypes for training data: 
    id        int64
    tweet    object
    label    object
    dtype: object
    
    Datatypes for Twitter data: 
    username        object
    description     object
    location        object
    following       object
    followers       object
    totaltweets     object
    retweetcount    object
    text            object
    hashtags        object
    dtype: object

Twitter API:n avulla haettu datasetti (tweet_df) vaatii hieman datatyyppien muuntelua.

```python
tweet_df = tweet_df.astype({'following': 'int32', 'followers': 'int32', 'totaltweets': 'int32', 'retweetcount': 'int32'})
print('New datatypes for Twitter data: \n' + str(tweet_df.dtypes) + '\n')
```

    New datatypes for Twitter data: 
    username        object
    description     object
    location        object
    following        int32
    followers        int32
    totaltweets      int32
    retweetcount     int32
    text            object
    hashtags        object
    dtype: object

Twiitit sisältävät tyypillisesti emojeita, linkkejä, viittauksia toisiin käyttäjiin ja hashtageja. Nämä pitää siistiä, ennen kuin sanoista luodaan featureita ja mallia aletaan opettamaan. Ennen tätä siivousta, katsotaan kuitenkin mitä opetus- ja validointidatasettien label-kolumnit pitävät sisällään.

```python
print('Training data labels: \n', train_df['label'].value_counts())
print('\nValidation data labels: \n', val_df['label'].value_counts())
```

    Training data labels: 
    real    3360
    fake    3060
    Name: label, dtype: int64
    
    Validation data labels: 
    real    1120
    fake    1020
    Name: label, dtype: int64

Datasetit ovat hyvin balanssissa. Nämä tekstimuotoiset arvot pitää kuitenkin binäärikoodata jatkoa varten.

Datasetit olivat yllättävän puhtaita. Mitään isompaa siivoamista ei sinänsä tarvinnut tehdä. Seuraavaksi katsaus jalostusvaiheeseen.

## Datan jalostaminen

Edellä huomattiin muutamia jalostustarpeita, kuten tekstin siistimisen tarve ja label-kolumnin binäärikoodaus. Tässä osiossa on muutamia koodiotteita siitä, miten jalostus on totetutettu.

Ensimmäisenä siivotaan twiitit. Poistetaan twiiteistä väli- ja erikoismerkit, kuten pisteet, sulut, pilkut yms. Poistetaan twiiteistä myös yleiset sanat, ns. stopwordit, kuten 'me', 'you', where' yms. Poistetaan myös kaikki linkit, viittaukset toisiin käyttäjiin sekä emojit. Muutetaan sanat myös perusmuotoisiksi ***WordNetLemmatizer()***-metodilla. Alla funktio, jolla tämä on toteutettu.

```python
def tweet_cleaner(tweets):
    for i in range(0, len(tweets)):
        tweet = tweets[i]

        emoji_pattern = re.compile(pattern = '['
            u'\U0001F600-\U0001F64F'  # emoticons
            u'\U0001F300-\U0001F5FF'  # symbols & pictographs
            u'\U0001F680-\U0001F6FF'  # transport & map symbols
            u'\U0001F1E0-\U0001F1FF'  # flags (iOS)
                           ']+', flags = re.UNICODE)

        tweet = html.unescape(tweet)    # Remove leftover HTML elements
        tweet = re.sub(r'@\w+', ' ', tweet) # Remove mentions to other people
        tweet = re.sub(r'http\S+', ' ', tweet)  # Remove links
        tweet = emoji_pattern.sub(r'', tweet)   # Remove emojis
        
        tweet = ''.join([punc for punc in tweet if not punc in puncs])   # Remove punctuation
        tweet = tweet.lower()   # Lowercase text
    
        tweetWord = tweet.split()   # Split to words

        lemmatiser = WordNetLemmatizer()
        tweetWord = [lemmatiser.lemmatize(word, pos='v') for word in tweetWord] # Lemmatize words

        tweets[i] = ''.join([word + ' ' for word in tweetWord if not word in stopws]) # Exclude stopwords
        
    return tweets 
```
  
Siivoamisen tuloksena twiitit, jotka sisälsivät esim. vain linkin, ovat tyhjiä. Poistetaan dataseteistä rivit, joissa siivottu twiitti on tyhjä merkkijono.

```python
train_df['clean_tweet'].replace('', np.nan, inplace=True)
train_df.dropna(subset=['clean_tweet'], inplace=True)
```

Binäärikodaataan validointi- ja opetusdatasettien label-kolumnin real ja fake arvot. Real = 1 ja fake = 0. Tämä totuusarvo tarkoittaa, onko twiitti totta vai tarua.

```python
train_df['is_real'] = pd.get_dummies(train_df['label'])['real']
val_df['is_real'] = pd.get_dummies(val_df['label'])['real']
val_df.head()
```
Katsaus validointidatasettiin jalostustoimenpiteiden jälkeen:
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>tweet</th>
      <th>label</th>
      <th>clean_tweet</th>
      <th>is_real</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>Chinese converting to Islam after realising th...</td>
      <td>fake</td>
      <td>chinese convert islam realise muslim affect co...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>11 out of 13 people (from the Diamond Princess...</td>
      <td>fake</td>
      <td>11 13 people diamond princess cruise ship inti...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>COVID-19 Is Caused By A Bacterium, Not Virus A...</td>
      <td>fake</td>
      <td>covid19 cause bacterium virus treat aspirin</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>Mike Pence in RNC speech praises Donald Trump’...</td>
      <td>fake</td>
      <td>mike pence rnc speech praise donald trump’s co...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>6/10 Sky's @EdConwaySky explains the latest #C...</td>
      <td>real</td>
      <td>610 sky explain latest covid19 data government...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

Koska neuroverkot eivät tyypillisesti käytä merkkijonoja syötteinään, on datasettejä jalostettava edelleen. Jokaiselle uniikille sanalle twiiteissä on määritettävä oma kokonaislukuarvo. Näistä kokonaisluvuista luodaan lista (sequence), jossa luvut ovat samassa järjestyksessä kuin niitä edustavat sanat ovat siivotussa twiitissä. Neuroverkon syötteiden on oltava samanmittaisia. Tämän vuoksi liian lyhyiden twiittien kokonaislukulistat pitää täydentää nollilla vastaamaan syötteelle asetettua pituutta.

Alla koodi, jolla tämä on tehty. Muuttuja *all_clean* pitää sisällään kaikki (10760) siivottua twiittiä.

```python
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_clean) # Map words to integers

def tokenize_tweet(tweets):
    texts = tokenizer.texts_to_sequences(tweets) # Convert clean_tweet to sequence of integers.
    texts = pad_sequences(texts, padding='post') # Pad the sequences with 0s so lenghts match.
    return texts

```

```python
texts_train = tokenize_tweet(train_df["clean_tweet"].copy()) # Collect the tweet sequences
train_df["tweet_sequence"] = list(texts_train)
train_df.head()
```

Katsaus opetusdatasettiin:
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>tweet</th>
      <th>label</th>
      <th>clean_tweet</th>
      <th>real</th>
      <th>is_real</th>
      <th>tweet_sequence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>The CDC currently reports 99031 deaths. In gen...</td>
      <td>real</td>
      <td>cdc currently report 99031 deaths general disc...</td>
      <td>1</td>
      <td>1</td>
      <td>[94, 199, 6, 8697, 10, 649, 4643, 89, 401, 378...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>States reported 1121 deaths a small rise from ...</td>
      <td>real</td>
      <td>state report 1121 deaths small rise last tuesd...</td>
      <td>1</td>
      <td>1</td>
      <td>[7, 6, 8699, 10, 650, 135, 47, 1044, 2487, 7, ...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>Politically Correct Woman (Almost) Uses Pandem...</td>
      <td>fake</td>
      <td>politically correct woman almost use pandemic ...</td>
      <td>0</td>
      <td>0</td>
      <td>[4644, 1155, 391, 473, 37, 23, 2488, 3889, 198...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>
      <td>real</td>
      <td>indiafightscorona 1524 covid test laboratories...</td>
      <td>1</td>
      <td>1</td>
      <td>[19, 8700, 14, 3, 211, 16, 3017, 213, 42, 5947...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>Populous states can generate large case counts...</td>
      <td>real</td>
      <td>populous state generate large case count look ...</td>
      <td>1</td>
      <td>1</td>
      <td>[5948, 7, 2112, 441, 2, 401, 194, 5, 2, 100, 5...</td>
    </tr>
  </tbody>
</table>
</div>

Lopuksi voidaan vielä varmistaa, että tekstin palauttaminen normaaliksi merkkijonoksi onnistuu:

```python
print('Some clean tweet:', train_df['clean_tweet'][300])
print('Same tweet\'s encoded sequence:', train_df['tweet_sequence'][300])
print('Sequence transformed back to normal text:', tokenizer.sequences_to_texts([train_df['tweet_sequence'][300]]))
```

    Some clean tweet: new numerous covid19 outbreaks recent cruise ship voyage extend previous sail order prevent spread covid19 among crew onboard 
    
    Same tweet's encoded sequence: [   5 6109    1  646  371 1328  820 8993  893  755 4773  275  141   31 1  367 2546 6110 ... 0    0    0]
    
    Sequence transformed back to normal text: ['new numerous covid19 outbreaks recent cruise ship voyage extend previous sail order prevent spread covid19 among crew onboard']

Jokainen sana, eli twiitin piirre, on halutussa muodossa, jotta malli voidaan tämän datasetin avulla opettaa. Myös opetus- ja validointidatasetin real ja fake arvot ovat binäärikoodattu mallia varten, joten mallin rakentamisen voi aloittaa.

## Hyödyllisiä linkkejä
1. [Esimerkki Jupyter Notebook](https://www.kaggle.com/lunamcbride24/covid19-tweet-truth-analysis)
2. [YouTube-video aiheesta](https://www.youtube.com/watch?v=j7EB7yeySDw)
3. [Hyvä blogipostaus](https://www.exxactcorp.com/blog/Deep-Learning/getting-started-with-nlp-using-the-tensorflow-and-keras-frameworks)

## Helppoa ja vaikeaa
1. En ole ennen toteuttanut NLP:tä käyttäen neuroverkkoja, joten datan jalostaminen oli hieman erilainen prosessi.
2. Twiittien siivousfunktion hiominen otti yllättävän paljon aikaa.
3. Googlettamalla tietoa tästä aiheesta löytyy paljon.